{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Assignment 2***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Import neccessary libraries and load the ratings***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings:    userId  movieId  rating  timestamp\n",
      "0       1        1     4.0  964982703\n",
      "1       1        3     4.0  964981247\n",
      "2       1        6     4.0  964982224\n",
      "3       1       47     5.0  964983815\n",
      "4       1       50     5.0  964982931\n",
      "Movies:    movieId                               title  \\\n",
      "0        1                    Toy Story (1995)   \n",
      "1        2                      Jumanji (1995)   \n",
      "2        3             Grumpier Old Men (1995)   \n",
      "3        4            Waiting to Exhale (1995)   \n",
      "4        5  Father of the Bride Part II (1995)   \n",
      "\n",
      "                                        genres  \n",
      "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
      "1                   Adventure|Children|Fantasy  \n",
      "2                               Comedy|Romance  \n",
      "3                         Comedy|Drama|Romance  \n",
      "4                                       Comedy  \n",
      "Tags:    userId  movieId              tag   timestamp\n",
      "0       2    60756            funny  1445714994\n",
      "1       2    60756  Highly quotable  1445714996\n",
      "2       2    60756     will ferrell  1445714992\n",
      "3       2    89774     Boxing story  1445715207\n",
      "4       2    89774              MMA  1445715200\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cargar ratings\n",
    "ratings = pd.read_csv('../../data/ml-latest-small/ratings.csv')\n",
    "print(\"Ratings:\", ratings.head())\n",
    "\n",
    "# Cargar movies\n",
    "movies = pd.read_csv('../../data/ml-latest-small/movies.csv')\n",
    "print(\"Movies:\", movies.head())\n",
    "\n",
    "# Cargar tags\n",
    "tags = pd.read_csv('../../data/ml-latest-small/tags.csv')\n",
    "print(\"Tags:\", tags.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Preproccess data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usuarios después de filtrar: 610\n",
      "Películas después de filtrar: 2269\n"
     ]
    }
   ],
   "source": [
    "# Filtrar usuarios y películas con menos de 10 ratings\n",
    "user_counts = ratings['userId'].value_counts()\n",
    "ratings = ratings[ratings['userId'].isin(user_counts[user_counts >= 10].index)]\n",
    "\n",
    "movie_counts = ratings['movieId'].value_counts()\n",
    "ratings = ratings[ratings['movieId'].isin(movie_counts[movie_counts >= 10].index)]\n",
    "\n",
    "print(\"Usuarios después de filtrar:\", ratings['userId'].nunique())\n",
    "print(\"Películas después de filtrar:\", ratings['movieId'].nunique())\n",
    "\n",
    "# Normalizar ratings a [0, 1]\n",
    "ratings['rating_norm'] = ratings['rating'] / 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create mappings for user ID's and films**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_user_ids = ratings['userId'].unique()\n",
    "unique_movie_ids = ratings['movieId'].unique()\n",
    "\n",
    "userId_to_index = {user_id: idx for idx, user_id in enumerate(unique_user_ids)}\n",
    "movieId_to_index = {movie_id: idx for idx, movie_id in enumerate(unique_movie_ids)}\n",
    "\n",
    "ratings['userIndex'] = ratings['userId'].map(userId_to_index)\n",
    "ratings['movieIndex'] = ratings['movieId'].map(movieId_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get the movie genres vector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para obtener todos los géneros\n",
    "def get_all_genres(movies_df):\n",
    "    genres_set = set()\n",
    "    for genres in movies_df['genres']:\n",
    "        for genre in genres.split(\"|\"):\n",
    "            genres_set.add(genre)\n",
    "    return list(genres_set)\n",
    "\n",
    "all_genres = get_all_genres(movies)\n",
    "genre_to_index = {genre: idx for idx, genre in enumerate(all_genres)}\n",
    "num_genres = len(all_genres)\n",
    "\n",
    "# Función para codificar los géneros en un vector one-hot\n",
    "def encode_genres(genres_str, genre_to_index, num_genres):\n",
    "    vec = np.zeros(num_genres, dtype=np.float32)\n",
    "    for genre in genres_str.split(\"|\"):\n",
    "        if genre in genre_to_index:\n",
    "            vec[genre_to_index[genre]] = 1.0\n",
    "    return vec\n",
    "\n",
    "# Agregar la columna 'genres_vector' al DataFrame movies\n",
    "movies['genres_vector'] = movies['genres'].apply(lambda x: encode_genres(x, genre_to_index, num_genres))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Procesar tags CSV para obtener un vector de características de tags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags features:    movieId                                       tag_features\n",
      "0        1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "1        2  [0.0011784274794497075, 0.0015045311115500077,...\n",
      "2        3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "3        5  [3.799389512103565e-07, 0.0004486458541472698,...\n",
      "4        7  [3.799389512103565e-07, 0.0004486458541472698,...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Agrupar los tags por movieId y concatenar todos los tags en un único string\n",
    "tags_grouped = tags.groupby('movieId')['tag'].apply(lambda x: \" \".join(x.astype(str))).reset_index()\n",
    "\n",
    "# Vectorizar el texto usando TF-IDF (limita el número de características, por ejemplo, a 100)\n",
    "vectorizer = TfidfVectorizer(max_features=100)\n",
    "tags_tfidf = vectorizer.fit_transform(tags_grouped['tag'])\n",
    "\n",
    "# Reducir dimensionalidad a un vector de tamaño fijo (por ejemplo, 32)\n",
    "svd = TruncatedSVD(n_components=32, random_state=42)\n",
    "tags_reduced = svd.fit_transform(tags_tfidf)\n",
    "\n",
    "# Crear un DataFrame con estas features\n",
    "tags_features_df = pd.DataFrame(tags_reduced, columns=[f'tag_feat_{i}' for i in range(32)])\n",
    "tags_features_df['movieId'] = tags_grouped['movieId']\n",
    "\n",
    "# Opcional: Crear una columna que contenga la lista de features\n",
    "tags_features_df['tag_features'] = tags_features_df[[f'tag_feat_{i}' for i in range(32)]].values.tolist()\n",
    "print(\"Tags features:\", tags_features_df[['movieId', 'tag_features']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge and export**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset fusionado:\n",
      "   userId  movieId  rating  timestamp  rating_norm  userIndex  movieIndex  \\\n",
      "0       1        1     4.0  964982703          0.8          0           0   \n",
      "1       1        3     4.0  964981247          0.8          0           1   \n",
      "2       1        6     4.0  964982224          0.8          0           2   \n",
      "3       1       47     5.0  964983815          1.0          0           3   \n",
      "4       1       50     5.0  964982931          1.0          0           4   \n",
      "\n",
      "                                       genres_vector  \\\n",
      "0  [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, ...   \n",
      "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "\n",
      "                                        tag_features  \n",
      "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "2                                                NaN  \n",
      "3  [5.323513928399962e-05, 0.02852995678619851, 0...  \n",
      "4  [7.195807280383005e-05, 0.027484829015544512, ...  \n"
     ]
    }
   ],
   "source": [
    "# Fusionar ratings con movies (para obtener genres_vector)\n",
    "merged_df = ratings.merge(movies[['movieId', 'genres_vector']], on='movieId', how='left')\n",
    "\n",
    "# Fusionar con los tags: solo tomamos movieId y tag_features\n",
    "merged_df = merged_df.merge(tags_features_df[['movieId', 'tag_features']], on='movieId', how='left')\n",
    "\n",
    "print(\"Dataset fusionado:\")\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Personalized Dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self, users, movies, ratings, movie_features, tag_features):\n",
    "        self.users = users\n",
    "        self.movies = movies\n",
    "        self.ratings = ratings\n",
    "        self.movie_features = movie_features  # Vector de géneros\n",
    "        self.tag_features = tag_features        # Vector de tags\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'user': self.users[idx],\n",
    "            'movie': self.movies[idx],\n",
    "            'rating': self.ratings[idx],\n",
    "            'movie_features': self.movie_features[idx],\n",
    "            'tag_features': self.tag_features[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Train Test Split***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 56505\n",
      "Validation size: 12164\n",
      "Test size: 12447\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Suponiendo que 'merged_df' es tu DataFrame fusionado (ratings + movies + tags)\n",
    "# Opcional: filtrar a usuarios con al menos 3 ratings\n",
    "user_counts = merged_df['userId'].value_counts()\n",
    "merged_filtered = merged_df[merged_df['userId'].isin(user_counts[user_counts >= 3].index)]\n",
    "\n",
    "# Dividir los datos por usuario para evitar que un mismo usuario aparezca en más de una partición\n",
    "train_list = []\n",
    "val_list = []\n",
    "test_list = []\n",
    "\n",
    "for user_id, group in merged_filtered.groupby('userId'):\n",
    "    # 70% para entrenamiento, 30% para temp\n",
    "    user_train, user_temp = train_test_split(group, test_size=0.30, random_state=42)\n",
    "    # Del 30% restante, 50% para validación y 50% para test (es decir, 15% cada uno)\n",
    "    user_val, user_test = train_test_split(user_temp, test_size=0.50, random_state=42)\n",
    "    train_list.append(user_train)\n",
    "    val_list.append(user_val)\n",
    "    test_list.append(user_test)\n",
    "\n",
    "train_data = pd.concat(train_list).reset_index(drop=True)\n",
    "val_data   = pd.concat(val_list).reset_index(drop=True)\n",
    "test_data  = pd.concat(test_list).reset_index(drop=True)\n",
    "\n",
    "print(\"Train size:\", len(train_data))\n",
    "print(\"Validation size:\", len(val_data))\n",
    "print(\"Test size:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def get_fixed_tag_features(x, length=32):\n",
    "    # Si x es una cadena, intenta evaluarla a lista\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            x = ast.literal_eval(x)\n",
    "        except Exception:\n",
    "            x = []\n",
    "    # Asegurarse de que x sea una lista\n",
    "    if not isinstance(x, list):\n",
    "        x = []\n",
    "    # Si la lista es más corta, se rellena con ceros; si es más larga, se trunca\n",
    "    if len(x) < length:\n",
    "        x = x + [0.0] * (length - len(x))\n",
    "    elif len(x) > length:\n",
    "        x = x[:length]\n",
    "    return x\n",
    "\n",
    "# Aplicar la función a la columna 'tag_features' para el conjunto de entrenamiento\n",
    "train_data['tag_features'] = train_data['tag_features'].apply(lambda x: get_fixed_tag_features(x, 32))\n",
    "# Hacer lo mismo para validación y test si es necesario\n",
    "val_data['tag_features'] = val_data['tag_features'].apply(lambda x: get_fixed_tag_features(x, 32))\n",
    "test_data['tag_features'] = test_data['tag_features'].apply(lambda x: get_fixed_tag_features(x, 32))\n",
    "\n",
    "# Ahora sí, puedes stackear las listas sin problemas:\n",
    "train_tag_features = torch.tensor(np.stack(train_data['tag_features'].values), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Dataloaders***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Ejemplo: cargar las columnas necesarias desde el DataFrame 'df'\n",
    "# Asegúrate de tener las particiones 'train_data', 'val_data' y 'test_data'\n",
    "\n",
    "# Convertir las columnas a tensores\n",
    "# Convertir a tensores para la partición de entrenamiento (ejemplo; haz lo mismo para val y test)\n",
    "train_user = torch.tensor(train_data['userIndex'].values, dtype=torch.long)\n",
    "train_movie = torch.tensor(train_data['movieIndex'].values, dtype=torch.long)\n",
    "train_rating = torch.tensor(train_data['rating_norm'].values, dtype=torch.float32)\n",
    "\n",
    "# Para columnas que contienen listas (asegúrate de que se lean como listas, no strings)\n",
    "train_movie_features = torch.tensor(np.stack(train_data['genres_vector'].values), dtype=torch.float32)\n",
    "train_tag_features = torch.tensor(np.stack(train_data['tag_features'].values), dtype=torch.float32)\n",
    "\n",
    "val_user = torch.tensor(val_data['userIndex'].values, dtype=torch.long)\n",
    "val_movie = torch.tensor(val_data['movieIndex'].values, dtype=torch.long)\n",
    "val_rating = torch.tensor(val_data['rating_norm'].values, dtype=torch.float32)\n",
    "val_movie_features = torch.tensor(np.stack(val_data['genres_vector'].values), dtype=torch.float32)\n",
    "val_tag_features = torch.tensor(np.stack(val_data['tag_features'].values), dtype=torch.float32)\n",
    "\n",
    "test_user = torch.tensor(test_data['userIndex'].values, dtype=torch.long)\n",
    "test_movie = torch.tensor(test_data['movieIndex'].values, dtype=torch.long)\n",
    "test_rating = torch.tensor(test_data['rating_norm'].values, dtype=torch.float32)\n",
    "test_movie_features = torch.tensor(np.stack(test_data['genres_vector'].values), dtype=torch.float32)\n",
    "test_tag_features = torch.tensor(np.stack(test_data['tag_features'].values), dtype=torch.float32)\n",
    "\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "train_dataset = MovieLensDataset(train_user, train_movie, train_rating, train_movie_features, train_tag_features)\n",
    "val_dataset = MovieLensDataset(val_user, val_movie, val_rating, val_movie_features, val_tag_features)\n",
    "test_dataset = MovieLensDataset(test_user, test_movie, test_rating, test_movie_features, test_tag_features)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Red Neuronal***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuralCollaborativeFilteringWithTags(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, genre_input_dim, tag_input_dim,\n",
    "                 embedding_dim=64, genre_emb_dim=32, tag_emb_dim=16, dropout_rate=0.3):\n",
    "        super(NeuralCollaborativeFilteringWithTags, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.movie_embedding = nn.Embedding(num_movies, embedding_dim)\n",
    "        \n",
    "        # Capa para transformar el vector de géneros\n",
    "        self.genre_layer = nn.Linear(genre_input_dim, genre_emb_dim)\n",
    "        # Capa para transformar el vector de tags\n",
    "        self.tag_layer = nn.Linear(tag_input_dim, tag_emb_dim)\n",
    "        \n",
    "        # Concatenación de: user_embedding, movie_embedding, genre_embedded y tag_embedded\n",
    "        input_dim = embedding_dim * 2 + genre_emb_dim + tag_emb_dim\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.output_layer = nn.Linear(64, 1)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, user, movie, movie_features, tag_features):\n",
    "        user_embedded = self.user_embedding(user)\n",
    "        movie_embedded = self.movie_embedding(movie)\n",
    "        genre_embedded = F.relu(self.genre_layer(movie_features))\n",
    "        tag_embedded = F.relu(self.tag_layer(tag_features))\n",
    "        \n",
    "        x = torch.cat([user_embedded, movie_embedded, genre_embedded, tag_embedded], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        out = self.output_layer(x)\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Red Neuronal***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.0853, Val Loss = 0.0409\n",
      "Epoch 2: Train Loss = 0.0545, Val Loss = 0.0375\n",
      "Epoch 3: Train Loss = 0.0494, Val Loss = 0.0358\n",
      "Epoch 4: Train Loss = 0.0467, Val Loss = 0.0348\n",
      "Epoch 5: Train Loss = 0.0447, Val Loss = 0.0346\n",
      "Epoch 6: Train Loss = 0.0430, Val Loss = 0.0336\n",
      "Epoch 7: Train Loss = 0.0420, Val Loss = 0.0333\n",
      "Epoch 8: Train Loss = 0.0408, Val Loss = 0.0334\n",
      "Epoch 9: Train Loss = 0.0400, Val Loss = 0.0324\n",
      "Epoch 10: Train Loss = 0.0388, Val Loss = 0.0327\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "num_users = len(userId_to_index)\n",
    "num_movies = len(movieId_to_index)\n",
    "# 'num_genres' es el número de géneros; 'tag_input_dim' es 32 (según la reducción de SVD)\n",
    "model = NeuralCollaborativeFilteringWithTags(num_users=num_users,\n",
    "                                              num_movies=num_movies,\n",
    "                                              genre_input_dim=num_genres,\n",
    "                                              tag_input_dim=32).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        users = batch['user'].to(device)\n",
    "        movies = batch['movie'].to(device)\n",
    "        ratings = batch['rating'].to(device)\n",
    "        movie_features = batch['movie_features'].to(device)\n",
    "        tag_features = batch['tag_features'].to(device)\n",
    "        \n",
    "        preds = model(users, movies, movie_features, tag_features)\n",
    "        loss = criterion(preds, ratings)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * len(ratings)\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_loader.dataset)\n",
    "    \n",
    "    # Validación\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            users = batch['user'].to(device)\n",
    "            movies = batch['movie'].to(device)\n",
    "            ratings = batch['rating'].to(device)\n",
    "            movie_features = batch['movie_features'].to(device)\n",
    "            tag_features = batch['tag_features'].to(device)\n",
    "            \n",
    "            preds = model(users, movies, movie_features, tag_features)\n",
    "            loss = criterion(preds, ratings)\n",
    "            val_loss += loss.item() * len(ratings)\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.915263\n",
      "MAE: 0.720493\n",
      "R2: 0.20794528722763062\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Se asume que ya tienes 'all_preds' y 'all_truth' obtenidos en el test (desnormalizados a escala original, ej. 0.5-5)\n",
    "all_preds = []\n",
    "all_truth = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        users = batch['user'].to(device)\n",
    "        movies = batch['movie'].to(device)\n",
    "        ratings = batch['rating'].to(device)\n",
    "        movie_features = batch['movie_features'].to(device)\n",
    "        tag_features = batch['tag_features'].to(device)\n",
    "        preds = model(users, movies, movie_features, tag_features)\n",
    "        # Desnormalizamos multiplicando por 5 (si esa es la escala original)\n",
    "        all_preds.extend((preds * 5).cpu().numpy())\n",
    "        all_truth.extend((ratings * 5).cpu().numpy())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_truth = np.array(all_truth)\n",
    "\n",
    "rmse = np.sqrt(np.mean((all_preds - all_truth) ** 2))\n",
    "mae = np.mean(np.abs(all_preds - all_truth))\n",
    "r2 = r2_score(all_truth, all_preds)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"R2:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@K: 0.5075409836065574\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "k = 10\n",
    "user_preds = defaultdict(list)\n",
    "user_truth = defaultdict(list)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        users = batch['user'].to(device)\n",
    "        movies = batch['movie'].to(device)\n",
    "        ratings = batch['rating'].to(device)\n",
    "        movie_features = batch['movie_features'].to(device)\n",
    "        tag_features = batch['tag_features'].to(device)\n",
    "        preds = model(users, movies, movie_features, tag_features)\n",
    "        # Desnormalizamos multiplicando por 5\n",
    "        for u, pred, true in zip(users.cpu().numpy(), (preds * 5).cpu().numpy(), (ratings * 5).cpu().numpy()):\n",
    "            user_preds[u].append(pred)\n",
    "            user_truth[u].append(true)\n",
    "\n",
    "precisions = []\n",
    "for u in user_preds:\n",
    "    preds_u = np.array(user_preds[u])\n",
    "    truths_u = np.array(user_truth[u])\n",
    "    # Ordenar índices según las predicciones (descendente)\n",
    "    top_k_indices = np.argsort(-preds_u)[:k]\n",
    "    # Consideramos relevante si el rating real es >= 4.0\n",
    "    relevant = (truths_u >= 4.0)\n",
    "    num_relevant = np.sum(relevant[top_k_indices])\n",
    "    precision_u = num_relevant / k\n",
    "    precisions.append(precision_u)\n",
    "\n",
    "precision_at_k = np.mean(precisions)\n",
    "print(\"Precision@K:\", precision_at_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG@K: 0.8579481092478046\n"
     ]
    }
   ],
   "source": [
    "def ndcg_at_k(relevances, k):\n",
    "    relevances = np.asarray(relevances)[:k]\n",
    "    if relevances.size == 0:\n",
    "        return 0.0\n",
    "    # DCG: usamos la fórmula (2^rel - 1) / log2(pos + 1)\n",
    "    dcg = np.sum((2 ** relevances - 1) / np.log2(np.arange(2, relevances.size + 2)))\n",
    "    # IDCG: orden ideal de relevancias\n",
    "    ideal_relevances = np.sort(relevances)[::-1]\n",
    "    idcg = np.sum((2 ** ideal_relevances - 1) / np.log2(np.arange(2, ideal_relevances.size + 2)))\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "ndcgs = []\n",
    "for u in user_preds:\n",
    "    preds_u = np.array(user_preds[u])\n",
    "    truths_u = np.array(user_truth[u])\n",
    "    # Convertir a relevancia binaria: 1 si rating >= 4.0, 0 de lo contrario\n",
    "    relevances = (truths_u >= 4.0).astype(int)\n",
    "    top_k_indices = np.argsort(-preds_u)[:k]\n",
    "    ndcg_u = ndcg_at_k(relevances[top_k_indices], k)\n",
    "    ndcgs.append(ndcg_u)\n",
    "\n",
    "ndcg_at_k_value = np.mean(ndcgs)\n",
    "print(\"NDCG@K:\", ndcg_at_k_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@K: 0.7664401057199333\n"
     ]
    }
   ],
   "source": [
    "recalls = []\n",
    "for u in user_preds:\n",
    "    preds_u = np.array(user_preds[u])\n",
    "    truths_u = np.array(user_truth[u])\n",
    "    # Número total de ítems relevantes para el usuario\n",
    "    total_relevant = np.sum(truths_u >= 4.0)\n",
    "    if total_relevant == 0:\n",
    "        continue  # O se podría asignar 0 para este usuario\n",
    "    top_k_indices = np.argsort(-preds_u)[:k]\n",
    "    relevant_top_k = np.sum(truths_u[top_k_indices] >= 4.0)\n",
    "    recall_u = relevant_top_k / total_relevant\n",
    "    recalls.append(recall_u)\n",
    "\n",
    "recall_at_k = np.mean(recalls) if recalls else 0.0\n",
    "print(\"Recall@K:\", recall_at_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1@10: 0.6107\n",
      "Entrada añadida a 'model_metrics.csv'\n"
     ]
    }
   ],
   "source": [
    "# Calcular F1@10 si ya tienes precision_at_k y recall_at_k\n",
    "f1_at_k = 2 * (precision_at_k * recall_at_k) / (precision_at_k + recall_at_k) if (precision_at_k + recall_at_k) > 0 else 0.0\n",
    "print(f\"F1@10: {f1_at_k:.4f}\")\n",
    "\n",
    "# Crear entrada para el CSV general\n",
    "metrics = {\n",
    "    \"Modelo\": \"R+M+T (100K)\",\n",
    "    \"Test RMSE\": float(np.round(rmse, 4)),\n",
    "    \"Test MAE\": float(np.round(mae, 4)),\n",
    "    \"Test R2\": float(np.round(r2, 4)),\n",
    "    \"Precision@10\": float(np.round(precision_at_k, 4)),\n",
    "    \"Recall@10\": float(np.round(recall_at_k, 4)),\n",
    "    \"F1@10\": float(np.round(f1_at_k, 4)),\n",
    "    \"NDCG@10\": float(np.round(ndcg_at_k_value, 4))\n",
    "    }\n",
    "\n",
    "# Guardar en CSV conjunto\n",
    "metrics_df = pd.DataFrame([metrics])\n",
    "csv_path = \"../../performance/model_evaluations.csv\"\n",
    "\n",
    "# Cargar CSV o crear uno nuevo si está vacío o no existe\n",
    "try:\n",
    "    all_metrics_df = pd.read_csv(csv_path)\n",
    "    if all_metrics_df.empty:\n",
    "        all_metrics_df = pd.DataFrame(columns=metrics_df.columns)\n",
    "except (FileNotFoundError, pd.errors.EmptyDataError):\n",
    "    all_metrics_df = pd.DataFrame(columns=metrics_df.columns)\n",
    "\n",
    "# Añadir y guardar\n",
    "all_metrics_df = pd.concat([all_metrics_df, metrics_df], ignore_index=True)\n",
    "all_metrics_df.to_csv(csv_path, index=False)\n",
    "print(\"Entrada añadida a 'model_metrics.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
