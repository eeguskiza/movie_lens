{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Assignment 2***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Import neccessary libraries and load the ratings***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  movieId  rating  timestamp\n",
      "0       1        1     4.0  964982703\n",
      "1       1        3     4.0  964981247\n",
      "2       1        6     4.0  964982224\n",
      "3       1       47     5.0  964983815\n",
      "4       1       50     5.0  964982931\n",
      "(100836, 4)\n",
      "Index(['userId', 'movieId', 'rating', 'timestamp'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el archivo ratings.csv\n",
    "ratings = pd.read_csv('../../data/ml-latest-small/ratings.csv')\n",
    "\n",
    "# Ver las primeras filas, tamaño y columnas\n",
    "print(ratings.head())\n",
    "print(ratings.shape)\n",
    "print(ratings.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Preproccessing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usuarios después de filtrar: 610\n",
      "Películas después de filtrar: 2269\n"
     ]
    }
   ],
   "source": [
    "# Filtrar usuarios con menos de 10 ratings\n",
    "user_counts = ratings['userId'].value_counts()\n",
    "ratings = ratings[ratings['userId'].isin(user_counts[user_counts >= 10].index)]\n",
    "\n",
    "# Filtrar películas con menos de 10 ratings\n",
    "movie_counts = ratings['movieId'].value_counts()\n",
    "ratings = ratings[ratings['movieId'].isin(movie_counts[movie_counts >= 10].index)]\n",
    "\n",
    "print(f\"Usuarios después de filtrar: {ratings['userId'].nunique()}\")\n",
    "print(f\"Películas después de filtrar: {ratings['movieId'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de usuarios únicos: 610\n",
      "Número de películas únicas: 2269\n",
      "   userId  movieId  rating  timestamp  userIndex  movieIndex\n",
      "0       1        1     4.0  964982703          0           0\n",
      "1       1        3     4.0  964981247          0           1\n",
      "2       1        6     4.0  964982224          0           2\n",
      "3       1       47     5.0  964983815          0           3\n",
      "4       1       50     5.0  964982931          0           4\n"
     ]
    }
   ],
   "source": [
    "# Obtener IDs únicos\n",
    "unique_user_ids = ratings['userId'].unique()\n",
    "unique_movie_ids = ratings['movieId'].unique()\n",
    "\n",
    "print(f\"Número de usuarios únicos: {len(unique_user_ids)}\")\n",
    "print(f\"Número de películas únicas: {len(unique_movie_ids)}\")\n",
    "\n",
    "# Crear diccionarios de mapeo\n",
    "userId_to_index = {user_id: idx for idx, user_id in enumerate(unique_user_ids)}\n",
    "movieId_to_index = {movie_id: idx for idx, movie_id in enumerate(unique_movie_ids)}\n",
    "\n",
    "# Aplicar el mapeo al DataFrame\n",
    "ratings['userIndex'] = ratings['userId'].map(userId_to_index)\n",
    "ratings['movieIndex'] = ratings['movieId'].map(movieId_to_index)\n",
    "\n",
    "# Comprobar\n",
    "print(ratings.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rating  rating_norm\n",
      "0     4.0          0.8\n",
      "1     4.0          0.8\n",
      "2     4.0          0.8\n",
      "3     5.0          1.0\n",
      "4     5.0          1.0\n"
     ]
    }
   ],
   "source": [
    "# Normalizamos ratings a [0, 1]\n",
    "ratings['rating_norm'] = ratings['rating'] / 5.0\n",
    "print(ratings[['rating', 'rating_norm']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Import Movies csv***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   movieId                               title  \\\n",
      "0        1                    Toy Story (1995)   \n",
      "1        2                      Jumanji (1995)   \n",
      "2        3             Grumpier Old Men (1995)   \n",
      "3        4            Waiting to Exhale (1995)   \n",
      "4        5  Father of the Bride Part II (1995)   \n",
      "\n",
      "                                        genres  \n",
      "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
      "1                   Adventure|Children|Fantasy  \n",
      "2                               Comedy|Romance  \n",
      "3                         Comedy|Drama|Romance  \n",
      "4                                       Comedy  \n"
     ]
    }
   ],
   "source": [
    "# Cargar el archivo movies.csv\n",
    "movies = pd.read_csv('../../data/ml-latest-small/movies.csv')\n",
    "print(movies.head())\n",
    "\n",
    "# Función para obtener todos los géneros existentes\n",
    "def get_all_genres(movies_df):\n",
    "    genres_set = set()\n",
    "    for genres in movies_df['genres']:\n",
    "        for genre in genres.split(\"|\"):\n",
    "            genres_set.add(genre)\n",
    "    return list(genres_set)\n",
    "\n",
    "all_genres = get_all_genres(movies)\n",
    "genre_to_index = {genre: idx for idx, genre in enumerate(all_genres)}\n",
    "num_genres = len(all_genres)\n",
    "\n",
    "# Función para codificar los géneros en un vector one-hot\n",
    "def encode_genres(genres_str, genre_to_index, num_genres):\n",
    "    vec = np.zeros(num_genres, dtype=np.float32)\n",
    "    for genre in genres_str.split(\"|\"):\n",
    "        if genre in genre_to_index:\n",
    "            vec[genre_to_index[genre]] = 1.0\n",
    "    return vec\n",
    "\n",
    "# Crear una nueva columna con el vector de géneros\n",
    "movies['genres_vector'] = movies['genres'].apply(lambda x: encode_genres(x, genre_to_index, num_genres))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Combine Datasets***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  movieId  rating  timestamp  userIndex  movieIndex  rating_norm  \\\n",
      "0       1        1     4.0  964982703          0           0          0.8   \n",
      "1       1        3     4.0  964981247          0           1          0.8   \n",
      "2       1        6     4.0  964982224          0           2          0.8   \n",
      "3       1       47     5.0  964983815          0           3          1.0   \n",
      "4       1       50     5.0  964982931          0           4          1.0   \n",
      "\n",
      "                                       genres_vector  \n",
      "0  [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "2  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, ...  \n",
      "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...  \n",
      "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, ...  \n"
     ]
    }
   ],
   "source": [
    "# Fusionar los datasets para agregar la información de géneros a cada rating\n",
    "ratings = ratings.merge(movies[['movieId', 'genres_vector']], on='movieId', how='left')\n",
    "print(ratings.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de usuarios únicos: 610\n",
      "Número de películas únicas: 2269\n",
      "   userId  movieId  rating  timestamp  userIndex  movieIndex  rating_norm  \\\n",
      "0       1        1     4.0  964982703          0           0          0.8   \n",
      "1       1        3     4.0  964981247          0           1          0.8   \n",
      "2       1        6     4.0  964982224          0           2          0.8   \n",
      "3       1       47     5.0  964983815          0           3          1.0   \n",
      "4       1       50     5.0  964982931          0           4          1.0   \n",
      "\n",
      "                                       genres_vector  \n",
      "0  [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "2  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, ...  \n",
      "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...  \n",
      "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, ...  \n"
     ]
    }
   ],
   "source": [
    "# Obtener IDs únicos y crear diccionarios de mapeo\n",
    "unique_user_ids = ratings['userId'].unique()\n",
    "unique_movie_ids = ratings['movieId'].unique()\n",
    "\n",
    "print(f\"Número de usuarios únicos: {len(unique_user_ids)}\")\n",
    "print(f\"Número de películas únicas: {len(unique_movie_ids)}\")\n",
    "\n",
    "userId_to_index = {user_id: idx for idx, user_id in enumerate(unique_user_ids)}\n",
    "movieId_to_index = {movie_id: idx for idx, movie_id in enumerate(unique_movie_ids)}\n",
    "\n",
    "# Aplicar el mapeo al DataFrame\n",
    "ratings['userIndex'] = ratings['userId'].map(userId_to_index)\n",
    "ratings['movieIndex'] = ratings['movieId'].map(movieId_to_index)\n",
    "print(ratings.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rating  rating_norm\n",
      "0     4.0          0.8\n",
      "1     4.0          0.8\n",
      "2     4.0          0.8\n",
      "3     5.0          1.0\n",
      "4     5.0          1.0\n"
     ]
    }
   ],
   "source": [
    "ratings['rating_norm'] = ratings['rating'] / 5.0\n",
    "print(ratings[['rating', 'rating_norm']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Personalized Dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self, users, movies, ratings, movie_features):\n",
    "        self.users = users\n",
    "        self.movies = movies\n",
    "        self.ratings = ratings\n",
    "        self.movie_features = movie_features  # vector de géneros (u otras features)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'user': self.users[idx],\n",
    "            'movie': self.movies[idx],\n",
    "            'rating': self.ratings[idx],\n",
    "            'movie_features': self.movie_features[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Train Test Validation Split***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 56505\n",
      "Validation size: 12164\n",
      "Test size: 12447\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Filtrar usuarios con al menos 3 ratings\n",
    "user_counts = ratings['userId'].value_counts()\n",
    "ratings_filtered = ratings[ratings['userId'].isin(user_counts[user_counts >= 3].index)]\n",
    "\n",
    "# Realizar el split por usuario\n",
    "train_list = []\n",
    "val_list = []\n",
    "test_list = []\n",
    "\n",
    "for user_id, group in ratings_filtered.groupby('userId'):\n",
    "    user_train, user_temp = train_test_split(group, test_size=0.30, random_state=42)\n",
    "    user_val, user_test = train_test_split(user_temp, test_size=0.50, random_state=42)\n",
    "    train_list.append(user_train)\n",
    "    val_list.append(user_val)\n",
    "    test_list.append(user_test)\n",
    "\n",
    "train_data = pd.concat(train_list).reset_index(drop=True)\n",
    "val_data = pd.concat(val_list).reset_index(drop=True)\n",
    "test_data = pd.concat(test_list).reset_index(drop=True)\n",
    "\n",
    "print(f\"Train size: {len(train_data)}\")\n",
    "print(f\"Validation size: {len(val_data)}\")\n",
    "print(f\"Test size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Dataloaders***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Convertir a tensores los índices y ratings (ya normalizados) como antes\n",
    "train_user = torch.tensor(train_data['userIndex'].values, dtype=torch.long)\n",
    "train_movie = torch.tensor(train_data['movieIndex'].values, dtype=torch.long)\n",
    "train_rating = torch.tensor(train_data['rating_norm'].values, dtype=torch.float32)\n",
    "\n",
    "val_user = torch.tensor(val_data['userIndex'].values, dtype=torch.long)\n",
    "val_movie = torch.tensor(val_data['movieIndex'].values, dtype=torch.long)\n",
    "val_rating = torch.tensor(val_data['rating_norm'].values, dtype=torch.float32)\n",
    "\n",
    "test_user = torch.tensor(test_data['userIndex'].values, dtype=torch.long)\n",
    "test_movie = torch.tensor(test_data['movieIndex'].values, dtype=torch.long)\n",
    "test_rating = torch.tensor(test_data['rating_norm'].values, dtype=torch.float32)\n",
    "\n",
    "# Convertir la columna 'genres_vector' (obtenida al procesar movies.csv) en tensores\n",
    "train_movie_features = torch.tensor(np.stack(train_data['genres_vector'].values), dtype=torch.float32)\n",
    "val_movie_features = torch.tensor(np.stack(val_data['genres_vector'].values), dtype=torch.float32)\n",
    "test_movie_features = torch.tensor(np.stack(test_data['genres_vector'].values), dtype=torch.float32)\n",
    "\n",
    "# Construcción de los datasets y dataloaders\n",
    "batch_size = 512\n",
    "\n",
    "train_dataset = MovieLensDataset(train_user, train_movie, train_rating, train_movie_features)\n",
    "val_dataset = MovieLensDataset(val_user, val_movie, val_rating, val_movie_features)\n",
    "test_dataset = MovieLensDataset(test_user, test_movie, test_rating, test_movie_features)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuralCollaborativeFiltering(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, genre_input_dim, embedding_dim=64, genre_emb_dim=32, dropout_rate=0.3):\n",
    "        super(NeuralCollaborativeFiltering, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.movie_embedding = nn.Embedding(num_movies, embedding_dim)\n",
    "        \n",
    "        # Capa para transformar el vector de géneros a un espacio de menor dimensión\n",
    "        self.genre_layer = nn.Linear(genre_input_dim, genre_emb_dim)\n",
    "        \n",
    "        # La entrada del MLP es la concatenación de: user_embedding, movie_embedding y genre_embedding\n",
    "        input_dim = embedding_dim * 2 + genre_emb_dim\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.output_layer = nn.Linear(64, 1)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, user, movie, movie_features):\n",
    "        user_embedded = self.user_embedding(user)\n",
    "        movie_embedded = self.movie_embedding(movie)\n",
    "        genre_embedded = F.relu(self.genre_layer(movie_features))\n",
    "        \n",
    "        x = torch.cat([user_embedded, movie_embedded, genre_embedded], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        out = self.output_layer(x)\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***GPU Usage***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "Modelo con integración de features de películas creado correctamente\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "num_users = len(userId_to_index)\n",
    "num_movies = len(movieId_to_index)\n",
    "# 'num_genres' es el número de géneros únicos obtenido al procesar movies.csv\n",
    "model = NeuralCollaborativeFiltering(num_users=num_users, num_movies=num_movies, genre_input_dim=num_genres).to(device)\n",
    "print(\"Modelo con integración de features de películas creado correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Loss Function and Optimizer***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Model Trainning***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.0761, Val Loss = 0.0401\n",
      "Epoch 2: Train Loss = 0.0499, Val Loss = 0.0374\n",
      "Epoch 3: Train Loss = 0.0454, Val Loss = 0.0358\n",
      "Epoch 4: Train Loss = 0.0433, Val Loss = 0.0360\n",
      "Epoch 5: Train Loss = 0.0414, Val Loss = 0.0348\n",
      "Epoch 6: Train Loss = 0.0403, Val Loss = 0.0342\n",
      "Epoch 7: Train Loss = 0.0391, Val Loss = 0.0341\n",
      "Epoch 8: Train Loss = 0.0382, Val Loss = 0.0335\n",
      "Epoch 9: Train Loss = 0.0373, Val Loss = 0.0327\n",
      "Epoch 10: Train Loss = 0.0365, Val Loss = 0.0328\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10  # Ajusta según convenga\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        users = batch['user'].to(device)\n",
    "        movies = batch['movie'].to(device)\n",
    "        ratings = batch['rating'].to(device)  # ya normalizado\n",
    "        movie_features = batch['movie_features'].to(device)\n",
    "        \n",
    "        preds = model(users, movies, movie_features)\n",
    "        loss = criterion(preds, ratings)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * len(ratings)\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_loader.dataset)\n",
    "    \n",
    "    # Validación\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            users = batch['user'].to(device)\n",
    "            movies = batch['movie'].to(device)\n",
    "            ratings = batch['rating'].to(device)\n",
    "            movie_features = batch['movie_features'].to(device)\n",
    "            \n",
    "            preds = model(users, movies, movie_features)\n",
    "            loss = criterion(preds, ratings)\n",
    "            val_loss += loss.item() * len(ratings)\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Model Evaluation***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***RMSE***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "all_truth = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        users = batch['user'].to(device)\n",
    "        movies = batch['movie'].to(device)\n",
    "        ratings = batch['rating'].to(device)\n",
    "        movie_features = batch['movie_features'].to(device)\n",
    "        preds = model(users, movies, movie_features)\n",
    "        # Desnormalizamos: ratings y predicciones a escala original (por ejemplo, 0.5-5)\n",
    "        all_preds.extend((preds * 5).cpu().numpy())\n",
    "        all_truth.extend((ratings * 5).cpu().numpy())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_truth = np.array(all_truth)\n",
    "\n",
    "rmse = np.sqrt(np.mean((all_preds - all_truth) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***MAE***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = np.mean(np.abs(all_preds - all_truth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***R-Square***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(all_truth, all_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Precision***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "k = 10\n",
    "user_preds = defaultdict(list)\n",
    "user_truth = defaultdict(list)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        users = batch['user'].to(device)\n",
    "        movies = batch['movie'].to(device)\n",
    "        ratings = batch['rating'].to(device)\n",
    "        movie_features = batch['movie_features'].to(device)\n",
    "        preds = model(users, movies, movie_features)\n",
    "        # Desnormalizamos: escala 0.5-5\n",
    "        for u, pred, true in zip(users.cpu().numpy(), (preds * 5).cpu().numpy(), (ratings * 5).cpu().numpy()):\n",
    "            user_preds[u].append(pred)\n",
    "            user_truth[u].append(true)\n",
    "\n",
    "precisions = []\n",
    "for u in user_preds:\n",
    "    preds_u = np.array(user_preds[u])\n",
    "    truths_u = np.array(user_truth[u])\n",
    "    # Ordenar índices según predicciones descendentes y tomar los top K\n",
    "    top_k_indices = np.argsort(-preds_u)[:k]\n",
    "    # Definir como relevante si el rating real es >= 4.0\n",
    "    relevant = (truths_u >= 4.0)\n",
    "    num_relevant = np.sum(relevant[top_k_indices])\n",
    "    precision_u = num_relevant / k\n",
    "    precisions.append(precision_u)\n",
    "\n",
    "precision_at_k = np.mean(precisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***NDCG@K***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_at_k(relevances, k):\n",
    "    relevances = np.asarray(relevances)[:k]\n",
    "    if relevances.size == 0:\n",
    "        return 0.0\n",
    "    # DCG: usamos la fórmula (2^rel - 1) / log2(pos + 1)\n",
    "    dcg = np.sum((2 ** relevances - 1) / np.log2(np.arange(2, relevances.size + 2)))\n",
    "    # IDCG: DCG ideal (orden perfecto)\n",
    "    ideal_relevances = np.sort(relevances)[::-1]\n",
    "    idcg = np.sum((2 ** ideal_relevances - 1) / np.log2(np.arange(2, ideal_relevances.size + 2)))\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "ndcgs = []\n",
    "for u in user_preds:\n",
    "    preds_u = np.array(user_preds[u])\n",
    "    truths_u = np.array(user_truth[u])\n",
    "    # Relevancia binaria: 1 si rating >= 4.0, 0 de lo contrario\n",
    "    relevances = (truths_u >= 4.0).astype(int)\n",
    "    top_k_indices = np.argsort(-preds_u)[:k]\n",
    "    ndcg_u = ndcg_at_k(relevances[top_k_indices], k)\n",
    "    ndcgs.append(ndcg_u)\n",
    "\n",
    "ndcg_at_k_value = np.mean(ndcgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recall f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bloque: Cálculo de Recall@10 y F1@10 para el modelo con features de películas\n",
    "k = 10\n",
    "recalls = []\n",
    "for u in user_preds:\n",
    "    preds_u = np.array(user_preds[u])\n",
    "    truths_u = np.array(user_truth[u])\n",
    "    # Obtenemos los índices de los top k con mayor predicción\n",
    "    top_k_indices = np.argsort(-preds_u)[:k]\n",
    "    # Definimos relevantes: true rating >= 4.0\n",
    "    relevant = (truths_u >= 4.0)\n",
    "    total_relevant = np.sum(relevant)\n",
    "    if total_relevant > 0:\n",
    "        recall_u = np.sum(relevant[top_k_indices]) / total_relevant\n",
    "        recalls.append(recall_u)\n",
    "    else:\n",
    "        recalls.append(0.0)\n",
    "recall_at_k = np.mean(recalls)\n",
    "\n",
    "# Calcular F1@10: media armónica de Precision@10 y Recall@10\n",
    "f1_at_k = 2 * (precision_at_k * recall_at_k) / (precision_at_k + recall_at_k) if (precision_at_k + recall_at_k) > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas añadidas correctamente al archivo 'model_metrics.csv'\n"
     ]
    }
   ],
   "source": [
    "# Bloque final para añadir métricas al CSV 'model_metrics.csv' con el formato correcto\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Crear el diccionario con las métricas actuales\n",
    "current_metrics = {\n",
    "    \"Modelo\": \"R+M (100K)\",  # Puedes modificarlo según el modelo\n",
    "    \"Test RMSE\": float(np.round(rmse, 4)),\n",
    "    \"Test MAE\": float(np.round(mae, 4)),\n",
    "    \"Test R2\": float(np.round(r2, 4)),\n",
    "    \"Precision@10\": float(np.round(precision_at_k, 4)),\n",
    "    \"Recall@10\": float(np.round(recall_at_k, 4)),\n",
    "    \"F1@10\": float(np.round(f1_at_k, 4)),\n",
    "    \"NDCG@10\": float(np.round(ndcg_at_k_value, 4)),\n",
    "}\n",
    "\n",
    "# Convertir a DataFrame\n",
    "current_metrics_df = pd.DataFrame([current_metrics])\n",
    "\n",
    "# Ruta al CSV existente\n",
    "metrics_csv = \"../../performance/model_evaluations.csv\"\n",
    "\n",
    "# Cargar o crear el CSV\n",
    "try:\n",
    "    all_metrics_df = pd.read_csv(metrics_csv)\n",
    "    if all_metrics_df.empty:\n",
    "        all_metrics_df = pd.DataFrame(columns=current_metrics_df.columns)\n",
    "except (FileNotFoundError, pd.errors.EmptyDataError):\n",
    "    all_metrics_df = pd.DataFrame(columns=current_metrics_df.columns)\n",
    "\n",
    "# Añadir nueva entrada y guardar\n",
    "all_metrics_df = pd.concat([all_metrics_df, current_metrics_df], ignore_index=True)\n",
    "all_metrics_df.to_csv(metrics_csv, index=False)\n",
    "\n",
    "print(\"Métricas añadidas correctamente al archivo 'model_metrics.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
