{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   UserID  MovieID  Rating  Timestamp\n",
      "0       1      122     5.0  838985046\n",
      "1       1      185     5.0  838983525\n",
      "2       1      231     5.0  838983392\n",
      "3       1      292     5.0  838983421\n",
      "4       1      316     5.0  838983392\n",
      "(10000054, 4)\n",
      "Index(['UserID', 'MovieID', 'Rating', 'Timestamp'], dtype='object')\n",
      "Usuarios después de filtrar: 69878\n",
      "Películas después de filtrar: 10196\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el archivo ratings.dat especificando el separador '::'\n",
    "ratings = pd.read_csv(\"../../data/ml-10m/ml-10M100K/ratings.dat\", sep=\"::\", engine=\"python\", header=None,\n",
    "                      names=[\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"])\n",
    "\n",
    "# Mostrar las primeras filas para verificar la carga\n",
    "print(ratings.head())\n",
    "print(ratings.shape)\n",
    "print(ratings.columns)\n",
    "\n",
    "# Filtro: usuarios con al menos 5 ratings\n",
    "user_counts = ratings['UserID'].value_counts()\n",
    "ratings = ratings[ratings['UserID'].isin(user_counts[user_counts >= 5].index)]\n",
    "\n",
    "# Filtro: películas con al menos 5 ratings\n",
    "movie_counts = ratings['MovieID'].value_counts()\n",
    "ratings = ratings[ratings['MovieID'].isin(movie_counts[movie_counts >= 5].index)]\n",
    "\n",
    "print(f\"Usuarios después de filtrar: {ratings['UserID'].nunique()}\")\n",
    "print(f\"Películas después de filtrar: {ratings['MovieID'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Rating  rating_norm  rating_std\n",
      "0     5.0          1.0    1.402784\n",
      "1     5.0          1.0    1.402784\n",
      "2     5.0          1.0    1.402784\n",
      "3     5.0          1.0    1.402784\n",
      "4     5.0          1.0    1.402784\n",
      "Media original: 3.5125 | Desviación: 1.0604\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Mapeo de IDs con LabelEncoder (más ordenado y reutilizable)\n",
    "user_encoder = LabelEncoder()\n",
    "movie_encoder = LabelEncoder()\n",
    "\n",
    "ratings['userIndex'] = user_encoder.fit_transform(ratings['UserID'])\n",
    "ratings['movieIndex'] = movie_encoder.fit_transform(ratings['MovieID'])\n",
    "\n",
    "# Normalización a [0, 1]\n",
    "ratings['rating_norm'] = ratings['Rating'] / 5.0\n",
    "\n",
    "# Estandarización (media 0, desviación 1)\n",
    "mean_rating = ratings['Rating'].mean()\n",
    "std_rating = ratings['Rating'].std()\n",
    "ratings['rating_std'] = (ratings['Rating'] - mean_rating) / std_rating\n",
    "\n",
    "# Mostrar resumen\n",
    "print(ratings[['Rating', 'rating_norm', 'rating_std']].head())\n",
    "print(f\"Media original: {mean_rating:.4f} | Desviación: {std_rating:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Timestamp            datetime  year  month  dayofweek\n",
      "0  838985046 1996-08-02 11:24:06  1996      8          4\n",
      "1  838983525 1996-08-02 10:58:45  1996      8          4\n",
      "2  838983392 1996-08-02 10:56:32  1996      8          4\n",
      "3  838983421 1996-08-02 10:57:01  1996      8          4\n",
      "4  838983392 1996-08-02 10:56:32  1996      8          4\n"
     ]
    }
   ],
   "source": [
    "# Convertir timestamp a datetime\n",
    "ratings['datetime'] = pd.to_datetime(ratings['Timestamp'], unit='s')\n",
    "\n",
    "# Extraer año, mes y día de la semana\n",
    "ratings['year'] = ratings['datetime'].dt.year\n",
    "ratings['month'] = ratings['datetime'].dt.month\n",
    "ratings['dayofweek'] = ratings['datetime'].dt.dayofweek  # 0=Lunes, 6=Domingo\n",
    "\n",
    "# Mostrar resumen\n",
    "print(ratings[['Timestamp', 'datetime', 'year', 'month', 'dayofweek']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 6968628\n",
      "Validation size: 1497394\n",
      "Test size: 1532794\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Elegir qué rating usar\n",
    "rating_col = 'rating_norm'  # cambia a 'rating_std' si quieres estandarizado\n",
    "\n",
    "# Filtrar usuarios con al menos 3 ratings para hacer split por usuario\n",
    "user_counts = ratings['UserID'].value_counts()\n",
    "ratings_filtered = ratings[ratings['UserID'].isin(user_counts[user_counts >= 3].index)]\n",
    "\n",
    "# Split por usuario\n",
    "train_list, val_list, test_list = [], [], []\n",
    "\n",
    "for user_id, group in ratings_filtered.groupby('UserID'):\n",
    "    user_train, user_temp = train_test_split(group, test_size=0.30, random_state=42)\n",
    "    user_val, user_test = train_test_split(user_temp, test_size=0.50, random_state=42)\n",
    "\n",
    "    train_list.append(user_train)\n",
    "    val_list.append(user_val)\n",
    "    test_list.append(user_test)\n",
    "\n",
    "train_data = pd.concat(train_list).reset_index(drop=True)\n",
    "val_data = pd.concat(val_list).reset_index(drop=True)\n",
    "test_data = pd.concat(test_list).reset_index(drop=True)\n",
    "\n",
    "print(f\"Train size: {len(train_data)}\")\n",
    "print(f\"Validation size: {len(val_data)}\")\n",
    "print(f\"Test size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Convertir a tensores\n",
    "train_user = torch.tensor(train_data['userIndex'].values, dtype=torch.long)\n",
    "train_movie = torch.tensor(train_data['movieIndex'].values, dtype=torch.long)\n",
    "train_rating = torch.tensor(train_data[rating_col].values, dtype=torch.float32)\n",
    "\n",
    "val_user = torch.tensor(val_data['userIndex'].values, dtype=torch.long)\n",
    "val_movie = torch.tensor(val_data['movieIndex'].values, dtype=torch.long)\n",
    "val_rating = torch.tensor(val_data[rating_col].values, dtype=torch.float32)\n",
    "\n",
    "test_user = torch.tensor(test_data['userIndex'].values, dtype=torch.long)\n",
    "test_movie = torch.tensor(test_data['movieIndex'].values, dtype=torch.long)\n",
    "test_rating = torch.tensor(test_data[rating_col].values, dtype=torch.float32)\n",
    "\n",
    "# Dataset personalizado\n",
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self, users, movies, ratings):\n",
    "        self.users = users\n",
    "        self.movies = movies\n",
    "        self.ratings = ratings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'user': self.users[idx],\n",
    "            'movie': self.movies[idx],\n",
    "            'rating': self.ratings[idx]\n",
    "        }\n",
    "\n",
    "# Crear datasets\n",
    "train_dataset = MovieLensDataset(train_user, train_movie, train_rating)\n",
    "val_dataset = MovieLensDataset(val_user, val_movie, val_rating)\n",
    "test_dataset = MovieLensDataset(test_user, test_movie, test_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 512  # Puedes ajustar esto según tu GPU\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuMF(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, embedding_dim_gmf=32, embedding_dim_mlp=32, dropout=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embeddings GMF\n",
    "        self.user_embedding_gmf = nn.Embedding(num_users, embedding_dim_gmf)\n",
    "        self.movie_embedding_gmf = nn.Embedding(num_movies, embedding_dim_gmf)\n",
    "\n",
    "        # Embeddings MLP\n",
    "        self.user_embedding_mlp = nn.Embedding(num_users, embedding_dim_mlp)\n",
    "        self.movie_embedding_mlp = nn.Embedding(num_movies, embedding_dim_mlp)\n",
    "\n",
    "        # MLP layers\n",
    "        self.fc1 = nn.Linear(embedding_dim_mlp * 2, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Output layer: combina GMF + MLP\n",
    "        self.output = nn.Linear(embedding_dim_gmf + 64, 1)\n",
    "\n",
    "    def forward(self, user, movie):\n",
    "        # GMF path\n",
    "        user_gmf = self.user_embedding_gmf(user)\n",
    "        movie_gmf = self.movie_embedding_gmf(movie)\n",
    "        gmf_output = user_gmf * movie_gmf  # Element-wise product\n",
    "\n",
    "        # MLP path\n",
    "        user_mlp = self.user_embedding_mlp(user)\n",
    "        movie_mlp = self.movie_embedding_mlp(movie)\n",
    "        mlp_input = torch.cat([user_mlp, movie_mlp], dim=1)\n",
    "        x = F.relu(self.bn1(self.fc1(mlp_input)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Combine GMF + MLP outputs\n",
    "        final_input = torch.cat([gmf_output, x], dim=1)\n",
    "        out = self.output(final_input)\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "model = NeuMF(\n",
    "    num_users=len(user_encoder.classes_),\n",
    "    num_movies=len(movie_encoder.classes_),\n",
    "    embedding_dim_gmf=32,\n",
    "    embedding_dim_mlp=32,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "# Función de pérdida\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizador\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Scheduler: baja el LR si la validación no mejora\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Train Loss: 0.0377 | Val Loss: 0.0308\n",
      "Epoch 2/30 | Train Loss: 0.0310 | Val Loss: 0.0301\n",
      "Epoch 3/30 | Train Loss: 0.0307 | Val Loss: 0.0299\n",
      "Epoch 4/30 | Train Loss: 0.0306 | Val Loss: 0.0298\n",
      "Epoch 5/30 | Train Loss: 0.0306 | Val Loss: 0.0299\n",
      "Epoch 6/30 | Train Loss: 0.0306 | Val Loss: 0.0299\n",
      "Epoch 7/30 | Train Loss: 0.0306 | Val Loss: 0.0300\n",
      "Epoch 8/30 | Train Loss: 0.0296 | Val Loss: 0.0288\n",
      "Epoch 9/30 | Train Loss: 0.0294 | Val Loss: 0.0286\n",
      "Epoch 10/30 | Train Loss: 0.0293 | Val Loss: 0.0285\n",
      "Epoch 11/30 | Train Loss: 0.0292 | Val Loss: 0.0284\n",
      "Epoch 12/30 | Train Loss: 0.0291 | Val Loss: 0.0283\n",
      "Epoch 13/30 | Train Loss: 0.0290 | Val Loss: 0.0282\n",
      "Epoch 14/30 | Train Loss: 0.0290 | Val Loss: 0.0283\n",
      "Epoch 15/30 | Train Loss: 0.0290 | Val Loss: 0.0282\n",
      "Epoch 16/30 | Train Loss: 0.0290 | Val Loss: 0.0283\n",
      "Epoch 17/30 | Train Loss: 0.0281 | Val Loss: 0.0273\n",
      "Epoch 18/30 | Train Loss: 0.0279 | Val Loss: 0.0273\n",
      "Epoch 19/30 | Train Loss: 0.0279 | Val Loss: 0.0273\n",
      "Epoch 20/30 | Train Loss: 0.0278 | Val Loss: 0.0273\n",
      "Epoch 21/30 | Train Loss: 0.0278 | Val Loss: 0.0273\n",
      "Epoch 22/30 | Train Loss: 0.0278 | Val Loss: 0.0272\n",
      "Epoch 23/30 | Train Loss: 0.0277 | Val Loss: 0.0272\n",
      "Epoch 24/30 | Train Loss: 0.0277 | Val Loss: 0.0272\n",
      "Epoch 25/30 | Train Loss: 0.0277 | Val Loss: 0.0272\n",
      "Epoch 26/30 | Train Loss: 0.0277 | Val Loss: 0.0271\n",
      "Epoch 27/30 | Train Loss: 0.0276 | Val Loss: 0.0272\n",
      "Epoch 28/30 | Train Loss: 0.0276 | Val Loss: 0.0271\n",
      "Epoch 29/30 | Train Loss: 0.0276 | Val Loss: 0.0271\n",
      "Epoch 30/30 | Train Loss: 0.0276 | Val Loss: 0.0271\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "early_stopping_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        users = batch['user'].to(device)\n",
    "        movies = batch['movie'].to(device)\n",
    "        ratings = batch['rating'].to(device)\n",
    "\n",
    "        preds = model(users, movies)\n",
    "        loss = criterion(preds, ratings)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * len(ratings)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader.dataset)\n",
    "\n",
    "    # VALIDACIÓN\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            users = batch['user'].to(device)\n",
    "            movies = batch['movie'].to(device)\n",
    "            ratings = batch['rating'].to(device)\n",
    "\n",
    "            preds = model(users, movies)\n",
    "            loss = criterion(preds, ratings)\n",
    "            val_loss += loss.item() * len(ratings)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # EARLY STOPPING (opcional)\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        early_stopping_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(\"🔁 Early stopping activado.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 MÉTRICAS DE ERROR - NeuMF\n",
      "✅ RMSE: 0.8239\n",
      "✅ MAE : 0.6384\n",
      "✅ R²  : 0.3970\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Cargar el mejor modelo entrenado\n",
    "model.eval()\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        users = batch['user'].to(device)\n",
    "        movies = batch['movie'].to(device)\n",
    "        ratings = batch['rating'].to(device)\n",
    "\n",
    "        preds = model(users, movies)\n",
    "\n",
    "        # Si los ratings están normalizados, desnormalizamos\n",
    "        preds = preds * 5\n",
    "        ratings = ratings * 5\n",
    "\n",
    "        y_true.extend(ratings.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "# Convertimos a arrays\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "# Cálculo de métricas\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print(\"📊 MÉTRICAS DE ERROR - NeuMF\")\n",
    "print(f\"✅ RMSE: {rmse:.4f}\")\n",
    "print(f\"✅ MAE : {mae:.4f}\")\n",
    "print(f\"✅ R²  : {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Precision@10: 0.5376\n",
      "📈 NDCG@10: 0.8899\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "k = 10\n",
    "user_preds = defaultdict(list)\n",
    "user_truth = defaultdict(list)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        users = batch['user'].to(device)\n",
    "        movies = batch['movie'].to(device)\n",
    "        ratings = batch['rating'].to(device)\n",
    "\n",
    "        preds = model(users, movies)\n",
    "\n",
    "        # Desnormalizamos (importante para comparar con escala original)\n",
    "        preds = preds * 5\n",
    "        ratings = ratings * 5\n",
    "\n",
    "        for u, pred, true in zip(users.cpu().numpy(), preds.cpu().numpy(), ratings.cpu().numpy()):\n",
    "            user_preds[u].append(pred)\n",
    "            user_truth[u].append(true)\n",
    "\n",
    "# Precision@K\n",
    "precisions = []\n",
    "for u in user_preds:\n",
    "    preds_u = np.array(user_preds[u])\n",
    "    truths_u = np.array(user_truth[u])\n",
    "\n",
    "    top_k_indices = np.argsort(-preds_u)[:k]\n",
    "    relevant = (truths_u >= 4.0)\n",
    "    num_relevant = np.sum(relevant[top_k_indices])\n",
    "\n",
    "    precision_u = num_relevant / k\n",
    "    precisions.append(precision_u)\n",
    "\n",
    "precision_at_k = np.mean(precisions)\n",
    "\n",
    "# NDCG@K\n",
    "def ndcg_at_k(relevances, k):\n",
    "    relevances = np.asarray(relevances)[:k]\n",
    "    if relevances.size == 0:\n",
    "        return 0.0\n",
    "    dcg = np.sum((2 ** relevances - 1) / np.log2(np.arange(2, relevances.size + 2)))\n",
    "    ideal_relevances = np.sort(relevances)[::-1]\n",
    "    idcg = np.sum((2 ** ideal_relevances - 1) / np.log2(np.arange(2, ideal_relevances.size + 2)))\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "ndcgs = []\n",
    "for u in user_preds:\n",
    "    preds_u = np.array(user_preds[u])\n",
    "    truths_u = np.array(user_truth[u])\n",
    "    relevances = (truths_u >= 4.0).astype(int)\n",
    "    top_k_indices = np.argsort(-preds_u)[:k]\n",
    "    ndcg_u = ndcg_at_k(relevances[top_k_indices], k)\n",
    "    ndcgs.append(ndcg_u)\n",
    "\n",
    "ndcg_at_k_value = np.mean(ndcgs)\n",
    "\n",
    "print(f\"🎯 Precision@{k}: {precision_at_k:.4f}\")\n",
    "print(f\"📈 NDCG@{k}: {ndcg_at_k_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Evaluación final:\n",
      "RMSE           : 0.8239\n",
      "MAE            : 0.6384\n",
      "R²             : 0.3970\n",
      "Precision@10  : 0.5376\n",
      "NDCG@10       : 0.8899\n",
      "Eval Time (s)  : 12.37\n",
      "✅ Métricas exportadas a 'neumf_1m_metrics_eval.csv'.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from collections import defaultdict\n",
    "\n",
    "# Cargar modelo entrenado\n",
    "model.eval()\n",
    "\n",
    "# Comenzar a contar el tiempo de evaluación\n",
    "start_time = time.time()\n",
    "\n",
    "# Evaluación sobre el set de test\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        users = batch['user'].to(device)\n",
    "        movies = batch['movie'].to(device)\n",
    "        ratings = batch['rating'].to(device)\n",
    "\n",
    "        preds = model(users, movies)\n",
    "\n",
    "        # Desnormalizar si se entrenó en [0,1]\n",
    "        preds = preds * 5\n",
    "        ratings = ratings * 5\n",
    "\n",
    "        y_true.extend(ratings.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "# Cálculo de Precision@10 y NDCG@10\n",
    "k = 10\n",
    "user_preds = defaultdict(list)\n",
    "user_truth = defaultdict(list)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        users = batch['user'].to(device)\n",
    "        movies = batch['movie'].to(device)\n",
    "        ratings = batch['rating'].to(device)\n",
    "        preds = model(users, movies)\n",
    "\n",
    "        preds = preds * 5\n",
    "        ratings = ratings * 5\n",
    "\n",
    "        for u, pred, true in zip(users.cpu().numpy(), preds.cpu().numpy(), ratings.cpu().numpy()):\n",
    "            user_preds[u].append(pred)\n",
    "            user_truth[u].append(true)\n",
    "\n",
    "precisions = []\n",
    "for u in user_preds:\n",
    "    preds_u = np.array(user_preds[u])\n",
    "    truths_u = np.array(user_truth[u])\n",
    "    top_k_indices = np.argsort(-preds_u)[:k]\n",
    "    relevant = (truths_u >= 4.0)\n",
    "    num_relevant = np.sum(relevant[top_k_indices])\n",
    "    precisions.append(num_relevant / k)\n",
    "precision_at_k = np.mean(precisions)\n",
    "\n",
    "def ndcg_at_k(relevances, k):\n",
    "    relevances = np.asarray(relevances)[:k]\n",
    "    if relevances.size == 0:\n",
    "        return 0.0\n",
    "    dcg = np.sum((2 ** relevances - 1) / np.log2(np.arange(2, relevances.size + 2)))\n",
    "    ideal_relevances = np.sort(relevances)[::-1]\n",
    "    idcg = np.sum((2 ** ideal_relevances - 1) / np.log2(np.arange(2, ideal_relevances.size + 2)))\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "ndcgs = []\n",
    "for u in user_preds:\n",
    "    preds_u = np.array(user_preds[u])\n",
    "    truths_u = np.array(user_truth[u])\n",
    "    relevances = (truths_u >= 4.0).astype(int)\n",
    "    top_k_indices = np.argsort(-preds_u)[:k]\n",
    "    ndcgs.append(ndcg_at_k(relevances[top_k_indices], k))\n",
    "ndcg_at_k_value = np.mean(ndcgs)\n",
    "\n",
    "# Tiempo total de evaluación\n",
    "eval_time = time.time() - start_time\n",
    "\n",
    "# Resultados\n",
    "print(\"🔍 Evaluación final:\")\n",
    "print(f\"RMSE           : {rmse:.4f}\")\n",
    "print(f\"MAE            : {mae:.4f}\")\n",
    "print(f\"R²             : {r2:.4f}\")\n",
    "print(f\"Precision@{k}  : {precision_at_k:.4f}\")\n",
    "print(f\"NDCG@{k}       : {ndcg_at_k_value:.4f}\")\n",
    "print(f\"Eval Time (s)  : {eval_time:.2f}\")\n",
    "\n",
    "# Guardar en CSV\n",
    "metrics = {\n",
    "    \"Model\": \"NeuMF (1M)\",\n",
    "    \"Test RMSE\": rmse,\n",
    "    \"Test MAE\": mae,\n",
    "    \"Test R2\": r2,\n",
    "    \"Precision@10\": precision_at_k,\n",
    "    \"NDCG@10\": ndcg_at_k_value,\n",
    "    \"Eval Time (s)\": eval_time\n",
    "}\n",
    "metrics_df = pd.DataFrame([metrics])\n",
    "metrics_df.to_csv(\"neumf_10m_metrics_eval.csv\", index=False)\n",
    "print(\"✅ Métricas exportadas a 'neumf_1m_metrics_eval.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
