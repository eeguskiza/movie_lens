{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Assignment 2***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Import neccessary libraries and load the ratings***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  movieId  rating  timestamp\n",
      "0       1        1     4.0  964982703\n",
      "1       1        3     4.0  964981247\n",
      "2       1        6     4.0  964982224\n",
      "3       1       47     5.0  964983815\n",
      "4       1       50     5.0  964982931\n",
      "(100836, 4)\n",
      "Index(['userId', 'movieId', 'rating', 'timestamp'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el archivo ratings.csv\n",
    "ratings = pd.read_csv('ml-latest-small/ratings.csv')\n",
    "\n",
    "# Ver las primeras filas\n",
    "print(ratings.head())\n",
    "\n",
    "# Revisar tamaño y columnas\n",
    "print(ratings.shape)\n",
    "print(ratings.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Preproccessing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usuarios después de filtrar: 610\n",
      "Películas después de filtrar: 2269\n"
     ]
    }
   ],
   "source": [
    "# Filtrar usuarios con menos de 10 ratings\n",
    "user_counts = ratings['userId'].value_counts()\n",
    "ratings = ratings[ratings['userId'].isin(user_counts[user_counts >= 10].index)]\n",
    "\n",
    "# Filtrar películas con menos de 10 ratings\n",
    "movie_counts = ratings['movieId'].value_counts()\n",
    "ratings = ratings[ratings['movieId'].isin(movie_counts[movie_counts >= 10].index)]\n",
    "\n",
    "print(f\"Usuarios después de filtrar: {ratings['userId'].nunique()}\")\n",
    "print(f\"Películas después de filtrar: {ratings['movieId'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de usuarios únicos: 610\n",
      "Número de películas únicas: 2269\n",
      "   userId  movieId  rating  timestamp  userIndex  movieIndex\n",
      "0       1        1     4.0  964982703          0           0\n",
      "1       1        3     4.0  964981247          0           1\n",
      "2       1        6     4.0  964982224          0           2\n",
      "3       1       47     5.0  964983815          0           3\n",
      "4       1       50     5.0  964982931          0           4\n"
     ]
    }
   ],
   "source": [
    "# Obtener IDs únicos\n",
    "unique_user_ids = ratings['userId'].unique()\n",
    "unique_movie_ids = ratings['movieId'].unique()\n",
    "\n",
    "print(f\"Número de usuarios únicos: {len(unique_user_ids)}\")\n",
    "print(f\"Número de películas únicas: {len(unique_movie_ids)}\")\n",
    "\n",
    "# Crear diccionarios de mapeo\n",
    "userId_to_index = {user_id: idx for idx, user_id in enumerate(unique_user_ids)}\n",
    "movieId_to_index = {movie_id: idx for idx, movie_id in enumerate(unique_movie_ids)}\n",
    "\n",
    "# Aplicar el mapeo al DataFrame\n",
    "ratings['userIndex'] = ratings['userId'].map(userId_to_index)\n",
    "ratings['movieIndex'] = ratings['movieId'].map(movieId_to_index)\n",
    "\n",
    "# Comprobar\n",
    "print(ratings.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rating  rating_norm\n",
      "0     4.0          0.8\n",
      "1     4.0          0.8\n",
      "2     4.0          0.8\n",
      "3     5.0          1.0\n",
      "4     5.0          1.0\n"
     ]
    }
   ],
   "source": [
    "# Normalizamos ratings a [0, 1]\n",
    "ratings['rating_norm'] = ratings['rating'] / 5.0\n",
    "print(ratings[['rating', 'rating_norm']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Split and Prepare***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 56505\n",
      "Validation size: 12164\n",
      "Test size: 12447\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Primero filtramos usuarios con al menos 3 ratings\n",
    "user_counts = ratings['userId'].value_counts()\n",
    "ratings_filtered = ratings[ratings['userId'].isin(user_counts[user_counts >= 3].index)]\n",
    "\n",
    "# Luego aplicamos el split\n",
    "train_list = []\n",
    "val_list = []\n",
    "test_list = []\n",
    "\n",
    "for user_id, group in ratings_filtered.groupby('userId'):\n",
    "    user_train, user_temp = train_test_split(group, test_size=0.30, random_state=42)\n",
    "    user_val, user_test = train_test_split(user_temp, test_size=0.50, random_state=42)\n",
    "    \n",
    "    train_list.append(user_train)\n",
    "    val_list.append(user_val)\n",
    "    test_list.append(user_test)\n",
    "\n",
    "train_data = pd.concat(train_list).reset_index(drop=True)\n",
    "val_data = pd.concat(val_list).reset_index(drop=True)\n",
    "test_data = pd.concat(test_list).reset_index(drop=True)\n",
    "\n",
    "print(f\"Train size: {len(train_data)}\")\n",
    "print(f\"Validation size: {len(val_data)}\")\n",
    "print(f\"Test size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Dataloaders***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Convertir a tensores los índices de usuario, película y ratings\n",
    "train_user = torch.tensor(train_data['userIndex'].values, dtype=torch.long)\n",
    "train_movie = torch.tensor(train_data['movieIndex'].values, dtype=torch.long)\n",
    "train_rating = torch.tensor(train_data['rating_norm'].values, dtype=torch.float32)\n",
    "\n",
    "val_user = torch.tensor(val_data['userIndex'].values, dtype=torch.long)\n",
    "val_movie = torch.tensor(val_data['movieIndex'].values, dtype=torch.long)\n",
    "val_rating = torch.tensor(val_data['rating_norm'].values, dtype=torch.float32)\n",
    "\n",
    "test_user = torch.tensor(test_data['userIndex'].values, dtype=torch.long)\n",
    "test_movie = torch.tensor(test_data['movieIndex'].values, dtype=torch.long)\n",
    "test_rating = torch.tensor(test_data['rating_norm'].values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Dataset Personalizado***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self, users, movies, ratings):\n",
    "        self.users = users\n",
    "        self.movies = movies\n",
    "        self.ratings = ratings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'user': self.users[idx],\n",
    "            'movie': self.movies[idx],\n",
    "            'rating': self.ratings[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Dataloaders***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "train_dataset = MovieLensDataset(train_user, train_movie, train_rating)\n",
    "val_dataset = MovieLensDataset(val_user, val_movie, val_rating)\n",
    "test_dataset = MovieLensDataset(test_user, test_movie, test_rating)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Simple Model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuralCollaborativeFiltering(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, embedding_dim=64, dropout_rate=0.3):\n",
    "        super(NeuralCollaborativeFiltering, self).__init__()\n",
    "        \n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.movie_embedding = nn.Embedding(num_movies, embedding_dim)\n",
    "        \n",
    "        # MLP con Dropout\n",
    "        self.fc1 = nn.Linear(embedding_dim * 2, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.output_layer = nn.Linear(64, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, user, movie):\n",
    "        user_embedded = self.user_embedding(user)\n",
    "        movie_embedded = self.movie_embedding(movie)\n",
    "        \n",
    "        x = torch.cat([user_embedded, movie_embedded], dim=1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        out = self.output_layer(x)\n",
    "        \n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Optimizer***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eeguskiza/miniconda3/envs/work/lib/python3.10/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo con Dropout creado correctamente\n"
     ]
    }
   ],
   "source": [
    "num_users = len(userId_to_index)\n",
    "num_movies = len(movieId_to_index)\n",
    "\n",
    "model = NeuralCollaborativeFiltering(num_users=num_users, num_movies=num_movies).to(device)\n",
    "print(\"Modelo con Dropout creado correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Trainning***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.0744, Val Loss = 0.0426\n",
      "Epoch 2: Train Loss = 0.0506, Val Loss = 0.0391\n",
      "Epoch 3: Train Loss = 0.0463, Val Loss = 0.0379\n",
      "Epoch 4: Train Loss = 0.0438, Val Loss = 0.0373\n",
      "Epoch 5: Train Loss = 0.0425, Val Loss = 0.0366\n",
      "Epoch 6: Train Loss = 0.0410, Val Loss = 0.0363\n",
      "Epoch 7: Train Loss = 0.0402, Val Loss = 0.0358\n",
      "Epoch 8: Train Loss = 0.0388, Val Loss = 0.0347\n",
      "Epoch 9: Train Loss = 0.0381, Val Loss = 0.0347\n",
      "Epoch 10: Train Loss = 0.0374, Val Loss = 0.0337\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10  # Puedes ajustar luego\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        users = batch['user'].to(device)\n",
    "        movies = batch['movie'].to(device)\n",
    "        ratings = batch['rating'].to(device)  # ← ya es rating_norm\n",
    "\n",
    "        preds = model(users, movies)\n",
    "        loss = criterion(preds, ratings)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * len(ratings)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            users = batch['user'].to(device)\n",
    "            movies = batch['movie'].to(device)\n",
    "            ratings = batch['rating'].to(device)\n",
    "\n",
    "            preds = model(users, movies)\n",
    "            loss = criterion(preds, ratings)\n",
    "            val_loss += loss.item() * len(ratings)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado correctamente.\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"solo_rantings.pth\")\n",
    "print(\"Modelo guardado correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralCollaborativeFiltering(num_users, num_movies)  # misma clase\n",
    "model.load_state_dict(torch.load(\"ncf_model_current.pth\"))\n",
    "model.to(device)  # si estabas en GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Model evaluation***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***RMSE***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'movie_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m movies \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovie\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m ratings \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 9\u001b[0m movie_features \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmovie_features\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m preds \u001b[38;5;241m=\u001b[39m model(users, movies, movie_features)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Desnormalizamos: ratings y predicciones a escala original (por ejemplo, 0.5-5)\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'movie_features'"
     ]
    }
   ],
   "source": [
    "all_preds = []\n",
    "all_truth = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        users = batch['user'].to(device)\n",
    "        movies = batch['movie'].to(device)\n",
    "        ratings = batch['rating'].to(device)\n",
    "        movie_features = batch['movie_features'].to(device)\n",
    "        preds = model(users, movies, movie_features)\n",
    "        # Desnormalizamos: ratings y predicciones a escala original (por ejemplo, 0.5-5)\n",
    "        all_preds.extend((preds * 5).cpu().numpy())\n",
    "        all_truth.extend((ratings * 5).cpu().numpy())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_truth = np.array(all_truth)\n",
    "\n",
    "rmse = np.sqrt(np.mean((all_preds - all_truth) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***MAE***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = np.mean(np.abs(all_preds - all_truth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***R-Square***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(all_truth, all_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Precision***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "k = 10\n",
    "user_preds = defaultdict(list)\n",
    "user_truth = defaultdict(list)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        users = batch['user'].to(device)\n",
    "        movies = batch['movie'].to(device)\n",
    "        ratings = batch['rating'].to(device)\n",
    "        movie_features = batch['movie_features'].to(device)\n",
    "        preds = model(users, movies, movie_features)\n",
    "        # Desnormalizamos: escala 0.5-5\n",
    "        for u, pred, true in zip(users.cpu().numpy(), (preds * 5).cpu().numpy(), (ratings * 5).cpu().numpy()):\n",
    "            user_preds[u].append(pred)\n",
    "            user_truth[u].append(true)\n",
    "\n",
    "precisions = []\n",
    "for u in user_preds:\n",
    "    preds_u = np.array(user_preds[u])\n",
    "    truths_u = np.array(user_truth[u])\n",
    "    # Ordenar índices según predicciones descendentes y tomar los top K\n",
    "    top_k_indices = np.argsort(-preds_u)[:k]\n",
    "    # Definir como relevante si el rating real es >= 4.0\n",
    "    relevant = (truths_u >= 4.0)\n",
    "    num_relevant = np.sum(relevant[top_k_indices])\n",
    "    precision_u = num_relevant / k\n",
    "    precisions.append(precision_u)\n",
    "\n",
    "precision_at_k = np.mean(precisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***NDCG@K***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'user_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dcg \u001b[38;5;241m/\u001b[39m idcg \u001b[38;5;28;01mif\u001b[39;00m idcg \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     12\u001b[0m ndcgs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m u \u001b[38;5;129;01min\u001b[39;00m \u001b[43muser_preds\u001b[49m:\n\u001b[1;32m     14\u001b[0m     preds_u \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(user_preds[u])\n\u001b[1;32m     15\u001b[0m     truths_u \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(user_truth[u])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'user_preds' is not defined"
     ]
    }
   ],
   "source": [
    "def ndcg_at_k(relevances, k):\n",
    "    relevances = np.asarray(relevances)[:k]\n",
    "    if relevances.size == 0:\n",
    "        return 0.0\n",
    "    # DCG: usamos la fórmula (2^rel - 1) / log2(pos + 1)\n",
    "    dcg = np.sum((2 ** relevances - 1) / np.log2(np.arange(2, relevances.size + 2)))\n",
    "    # IDCG: DCG ideal (orden perfecto)\n",
    "    ideal_relevances = np.sort(relevances)[::-1]\n",
    "    idcg = np.sum((2 ** ideal_relevances - 1) / np.log2(np.arange(2, ideal_relevances.size + 2)))\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "ndcgs = []\n",
    "for u in user_preds:\n",
    "    preds_u = np.array(user_preds[u])\n",
    "    truths_u = np.array(user_truth[u])\n",
    "    # Relevancia binaria: 1 si rating >= 4.0, 0 de lo contrario\n",
    "    relevances = (truths_u >= 4.0).astype(int)\n",
    "    top_k_indices = np.argsort(-preds_u)[:k]\n",
    "    ndcg_u = ndcg_at_k(relevances[top_k_indices], k)\n",
    "    ndcgs.append(ndcg_u)\n",
    "\n",
    "ndcg_at_k_value = np.mean(ndcgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***All Metrics***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Suponiendo que ya tienes las variables calculadas:\n",
    "# rmse, mae, r2, precision_at_k, ndcg_at_k_value\n",
    "\n",
    "# Aseguramos que todos los valores sean Python floats y estén redondeados\n",
    "metrics = {\n",
    "    'RMSE': float(np.round(rmse, 4)),\n",
    "    'MAE': float(np.round(mae, 4)),\n",
    "    'R2': float(np.round(r2, 4)),\n",
    "    'Pre': float(np.round(precision_at_k, 4)),\n",
    "    'NDCG@10': float(np.round(ndcg_at_k_value, 4))\n",
    "}\n",
    "\n",
    "# Crear un DataFrame\n",
    "metrics_df = pd.DataFrame(list(metrics.items()), columns=['Métrica', 'Valor'])\n",
    "\n",
    "# Alternativa: formatear explícitamente los valores en el DataFrame\n",
    "metrics_df['Valor'] = metrics_df['Valor'].apply(lambda x: f\"{x:.4f}\")\n",
    "\n",
    "# Generar y mostrar la tabla con un formato más visual\n",
    "tabla_formateada = tabulate(metrics_df, headers='keys', tablefmt='pretty', showindex=False)\n",
    "print(tabla_formateada)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
