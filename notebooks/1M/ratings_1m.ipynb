{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   UserID  MovieID  Rating  Timestamp\n",
      "0       1     1193       5  978300760\n",
      "1       1      661       3  978302109\n",
      "2       1      914       3  978301968\n",
      "3       1     3408       4  978300275\n",
      "4       1     2355       5  978824291\n",
      "(1000209, 4)\n",
      "Index(['UserID', 'MovieID', 'Rating', 'Timestamp'], dtype='object')\n",
      "Usuarios después de filtrar: 6040\n",
      "Películas después de filtrar: 3416\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el archivo ratings.dat especificando el separador '::'\n",
    "ratings = pd.read_csv(\"../../data/ml-1m/ratings.dat\", sep=\"::\", engine=\"python\", header=None,\n",
    "                      names=[\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"])\n",
    "\n",
    "# Mostrar las primeras filas para verificar la carga\n",
    "print(ratings.head())\n",
    "print(ratings.shape)\n",
    "print(ratings.columns)\n",
    "\n",
    "# Filtro: usuarios con al menos 5 ratings\n",
    "user_counts = ratings['UserID'].value_counts()\n",
    "ratings = ratings[ratings['UserID'].isin(user_counts[user_counts >= 5].index)]\n",
    "\n",
    "# Filtro: películas con al menos 5 ratings\n",
    "movie_counts = ratings['MovieID'].value_counts()\n",
    "ratings = ratings[ratings['MovieID'].isin(movie_counts[movie_counts >= 5].index)]\n",
    "\n",
    "print(f\"Usuarios después de filtrar: {ratings['UserID'].nunique()}\")\n",
    "print(f\"Películas después de filtrar: {ratings['MovieID'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Rating  rating_norm  rating_std\n",
      "0       5          1.0    1.269615\n",
      "1       3          0.6   -0.521065\n",
      "2       3          0.6   -0.521065\n",
      "3       4          0.8    0.374275\n",
      "4       5          1.0    1.269615\n",
      "Media original: 3.5820 | Desviación: 1.1169\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Mapeo de IDs con LabelEncoder (más ordenado y reutilizable)\n",
    "user_encoder = LabelEncoder()\n",
    "movie_encoder = LabelEncoder()\n",
    "\n",
    "ratings['userIndex'] = user_encoder.fit_transform(ratings['UserID'])\n",
    "ratings['movieIndex'] = movie_encoder.fit_transform(ratings['MovieID'])\n",
    "\n",
    "# Normalización a [0, 1]\n",
    "ratings['rating_norm'] = ratings['Rating'] / 5.0\n",
    "\n",
    "# Estandarización (media 0, desviación 1)\n",
    "mean_rating = ratings['Rating'].mean()\n",
    "std_rating = ratings['Rating'].std()\n",
    "ratings['rating_std'] = (ratings['Rating'] - mean_rating) / std_rating\n",
    "\n",
    "# Mostrar resumen\n",
    "print(ratings[['Rating', 'rating_norm', 'rating_std']].head())\n",
    "print(f\"Media original: {mean_rating:.4f} | Desviación: {std_rating:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Timestamp            datetime  year  month  dayofweek\n",
      "0  978300760 2000-12-31 22:12:40  2000     12          6\n",
      "1  978302109 2000-12-31 22:35:09  2000     12          6\n",
      "2  978301968 2000-12-31 22:32:48  2000     12          6\n",
      "3  978300275 2000-12-31 22:04:35  2000     12          6\n",
      "4  978824291 2001-01-06 23:38:11  2001      1          5\n"
     ]
    }
   ],
   "source": [
    "# Convertir timestamp a datetime\n",
    "ratings['datetime'] = pd.to_datetime(ratings['Timestamp'], unit='s')\n",
    "\n",
    "# Extraer año, mes y día de la semana\n",
    "ratings['year'] = ratings['datetime'].dt.year\n",
    "ratings['month'] = ratings['datetime'].dt.month\n",
    "ratings['dayofweek'] = ratings['datetime'].dt.dayofweek  # 0=Lunes, 6=Domingo\n",
    "\n",
    "# Mostrar resumen\n",
    "print(ratings[['Timestamp', 'datetime', 'year', 'month', 'dayofweek']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 697017\n",
      "Validation size: 149779\n",
      "Test size: 152815\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Elegir qué rating usar\n",
    "rating_col = 'rating_norm'  # cambia a 'rating_std' si quieres estandarizado\n",
    "\n",
    "# Filtrar usuarios con al menos 3 ratings para hacer split por usuario\n",
    "user_counts = ratings['UserID'].value_counts()\n",
    "ratings_filtered = ratings[ratings['UserID'].isin(user_counts[user_counts >= 3].index)]\n",
    "\n",
    "# Split por usuario\n",
    "train_list, val_list, test_list = [], [], []\n",
    "\n",
    "for user_id, group in ratings_filtered.groupby('UserID'):\n",
    "    user_train, user_temp = train_test_split(group, test_size=0.30, random_state=42)\n",
    "    user_val, user_test = train_test_split(user_temp, test_size=0.50, random_state=42)\n",
    "\n",
    "    train_list.append(user_train)\n",
    "    val_list.append(user_val)\n",
    "    test_list.append(user_test)\n",
    "\n",
    "train_data = pd.concat(train_list).reset_index(drop=True)\n",
    "val_data = pd.concat(val_list).reset_index(drop=True)\n",
    "test_data = pd.concat(test_list).reset_index(drop=True)\n",
    "\n",
    "print(f\"Train size: {len(train_data)}\")\n",
    "print(f\"Validation size: {len(val_data)}\")\n",
    "print(f\"Test size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Convertir a tensores\n",
    "train_user = torch.tensor(train_data['userIndex'].values, dtype=torch.long)\n",
    "train_movie = torch.tensor(train_data['movieIndex'].values, dtype=torch.long)\n",
    "train_rating = torch.tensor(train_data[rating_col].values, dtype=torch.float32)\n",
    "\n",
    "val_user = torch.tensor(val_data['userIndex'].values, dtype=torch.long)\n",
    "val_movie = torch.tensor(val_data['movieIndex'].values, dtype=torch.long)\n",
    "val_rating = torch.tensor(val_data[rating_col].values, dtype=torch.float32)\n",
    "\n",
    "test_user = torch.tensor(test_data['userIndex'].values, dtype=torch.long)\n",
    "test_movie = torch.tensor(test_data['movieIndex'].values, dtype=torch.long)\n",
    "test_rating = torch.tensor(test_data[rating_col].values, dtype=torch.float32)\n",
    "\n",
    "# Dataset personalizado\n",
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self, users, movies, ratings):\n",
    "        self.users = users\n",
    "        self.movies = movies\n",
    "        self.ratings = ratings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'user': self.users[idx],\n",
    "            'movie': self.movies[idx],\n",
    "            'rating': self.ratings[idx]\n",
    "        }\n",
    "\n",
    "# Crear datasets\n",
    "train_dataset = MovieLensDataset(train_user, train_movie, train_rating)\n",
    "val_dataset = MovieLensDataset(val_user, val_movie, val_rating)\n",
    "test_dataset = MovieLensDataset(test_user, test_movie, test_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 512  # Puedes ajustar esto según tu GPU\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuMF(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, embedding_dim_gmf=32, embedding_dim_mlp=32, dropout=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embeddings GMF\n",
    "        self.user_embedding_gmf = nn.Embedding(num_users, embedding_dim_gmf)\n",
    "        self.movie_embedding_gmf = nn.Embedding(num_movies, embedding_dim_gmf)\n",
    "\n",
    "        # Embeddings MLP\n",
    "        self.user_embedding_mlp = nn.Embedding(num_users, embedding_dim_mlp)\n",
    "        self.movie_embedding_mlp = nn.Embedding(num_movies, embedding_dim_mlp)\n",
    "\n",
    "        # MLP layers\n",
    "        self.fc1 = nn.Linear(embedding_dim_mlp * 2, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Output layer: combina GMF + MLP\n",
    "        self.output = nn.Linear(embedding_dim_gmf + 64, 1)\n",
    "\n",
    "    def forward(self, user, movie):\n",
    "        # GMF path\n",
    "        user_gmf = self.user_embedding_gmf(user)\n",
    "        movie_gmf = self.movie_embedding_gmf(movie)\n",
    "        gmf_output = user_gmf * movie_gmf  # Element-wise product\n",
    "\n",
    "        # MLP path\n",
    "        user_mlp = self.user_embedding_mlp(user)\n",
    "        movie_mlp = self.movie_embedding_mlp(movie)\n",
    "        mlp_input = torch.cat([user_mlp, movie_mlp], dim=1)\n",
    "        x = F.relu(self.bn1(self.fc1(mlp_input)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Combine GMF + MLP outputs\n",
    "        final_input = torch.cat([gmf_output, x], dim=1)\n",
    "        out = self.output(final_input)\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# Aumentar dimensión de embeddings y añadir dropout adaptativo\n",
    "model = NeuMF(\n",
    "    num_users=len(user_encoder.classes_),\n",
    "    num_movies=len(movie_encoder.classes_),\n",
    "    embedding_dim_gmf=64,  # ↑ de 32 a 64\n",
    "    embedding_dim_mlp=64,\n",
    "    dropout=0.5  # Regularización más agresiva\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# Función de pérdida\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizador\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.002, weight_decay=1e-6)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=0.005,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    epochs=30\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Train Loss: 0.1523 | Val Loss: 0.1040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eeguskiza/miniconda3/envs/work/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 | Train Loss: 0.0660 | Val Loss: 0.0822\n",
      "Epoch 3/30 | Train Loss: 0.0575 | Val Loss: 0.0631\n",
      "Epoch 4/30 | Train Loss: 0.0515 | Val Loss: 0.0520\n",
      "Epoch 5/30 | Train Loss: 0.0479 | Val Loss: 0.0470\n",
      "Epoch 6/30 | Train Loss: 0.0455 | Val Loss: 0.0443\n",
      "Epoch 7/30 | Train Loss: 0.0436 | Val Loss: 0.0421\n",
      "Epoch 8/30 | Train Loss: 0.0419 | Val Loss: 0.0408\n",
      "Epoch 9/30 | Train Loss: 0.0406 | Val Loss: 0.0399\n",
      "Epoch 10/30 | Train Loss: 0.0394 | Val Loss: 0.0387\n",
      "Epoch 11/30 | Train Loss: 0.0386 | Val Loss: 0.0377\n",
      "Epoch 12/30 | Train Loss: 0.0378 | Val Loss: 0.0376\n",
      "Epoch 13/30 | Train Loss: 0.0372 | Val Loss: 0.0370\n",
      "Epoch 14/30 | Train Loss: 0.0367 | Val Loss: 0.0365\n",
      "Epoch 15/30 | Train Loss: 0.0363 | Val Loss: 0.0359\n",
      "Epoch 16/30 | Train Loss: 0.0359 | Val Loss: 0.0356\n",
      "Epoch 17/30 | Train Loss: 0.0356 | Val Loss: 0.0356\n",
      "Epoch 18/30 | Train Loss: 0.0354 | Val Loss: 0.0353\n",
      "Epoch 19/30 | Train Loss: 0.0352 | Val Loss: 0.0352\n",
      "Epoch 20/30 | Train Loss: 0.0349 | Val Loss: 0.0350\n",
      "Epoch 21/30 | Train Loss: 0.0348 | Val Loss: 0.0347\n",
      "Epoch 22/30 | Train Loss: 0.0346 | Val Loss: 0.0349\n",
      "Epoch 23/30 | Train Loss: 0.0344 | Val Loss: 0.0348\n",
      "Epoch 24/30 | Train Loss: 0.0342 | Val Loss: 0.0346\n",
      "Epoch 25/30 | Train Loss: 0.0341 | Val Loss: 0.0346\n",
      "Epoch 26/30 | Train Loss: 0.0340 | Val Loss: 0.0346\n",
      "Epoch 27/30 | Train Loss: 0.0338 | Val Loss: 0.0345\n",
      "Epoch 28/30 | Train Loss: 0.0337 | Val Loss: 0.0344\n",
      "Epoch 29/30 | Train Loss: 0.0336 | Val Loss: 0.0345\n",
      "Epoch 30/30 | Train Loss: 0.0335 | Val Loss: 0.0345\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "early_stopping_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        users = batch['user'].to(device)\n",
    "        movies = batch['movie'].to(device)\n",
    "        ratings = batch['rating'].to(device)\n",
    "\n",
    "        preds = model(users, movies)\n",
    "        loss = criterion(preds, ratings)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * len(ratings)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader.dataset)\n",
    "\n",
    "    # VALIDACIÓN\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            users = batch['user'].to(device)\n",
    "            movies = batch['movie'].to(device)\n",
    "            ratings = batch['rating'].to(device)\n",
    "\n",
    "            preds = model(users, movies)\n",
    "            loss = criterion(preds, ratings)\n",
    "            val_loss += loss.item() * len(ratings)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # EARLY STOPPING (opcional)\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        early_stopping_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(\"Early stopping activado.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 MÉTRICAS DE ERROR - NeuMF\n",
      "✅ RMSE: 0.9285\n",
      "✅ MAE : 0.7381\n",
      "✅ R²  : 0.3092\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Cargar el mejor modelo entrenado\n",
    "model.eval()\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        users = batch['user'].to(device)\n",
    "        movies = batch['movie'].to(device)\n",
    "        ratings = batch['rating'].to(device)\n",
    "\n",
    "        preds = model(users, movies)\n",
    "\n",
    "        # Si los ratings están normalizados, desnormalizamos\n",
    "        preds = preds * 5\n",
    "        ratings = ratings * 5\n",
    "\n",
    "        y_true.extend(ratings.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "# Convertimos a arrays\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "# Cálculo de métricas\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print(\"📊 MÉTRICAS DE ERROR - NeuMF\")\n",
    "print(f\"✅ RMSE: {rmse:.4f}\")\n",
    "print(f\"✅ MAE : {mae:.4f}\")\n",
    "print(f\"✅ R²  : {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Precision@10: 0.6266\n",
      "📈 NDCG@10: 0.9125\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "k = 10\n",
    "user_preds = defaultdict(list)\n",
    "user_truth = defaultdict(list)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        users = batch['user'].to(device)\n",
    "        movies = batch['movie'].to(device)\n",
    "        ratings = batch['rating'].to(device)\n",
    "\n",
    "        preds = model(users, movies)\n",
    "\n",
    "        # Desnormalizamos (importante para comparar con escala original)\n",
    "        preds = preds * 5\n",
    "        ratings = ratings * 5\n",
    "\n",
    "        for u, pred, true in zip(users.cpu().numpy(), preds.cpu().numpy(), ratings.cpu().numpy()):\n",
    "            user_preds[u].append(pred)\n",
    "            user_truth[u].append(true)\n",
    "\n",
    "# Precision@K\n",
    "precisions = []\n",
    "for u in user_preds:\n",
    "    preds_u = np.array(user_preds[u])\n",
    "    truths_u = np.array(user_truth[u])\n",
    "\n",
    "    top_k_indices = np.argsort(-preds_u)[:k]\n",
    "    relevant = (truths_u >= 4.0)\n",
    "    num_relevant = np.sum(relevant[top_k_indices])\n",
    "\n",
    "    precision_u = num_relevant / k\n",
    "    precisions.append(precision_u)\n",
    "\n",
    "precision_at_k = np.mean(precisions)\n",
    "\n",
    "# NDCG@K\n",
    "def ndcg_at_k(relevances, k):\n",
    "    relevances = np.asarray(relevances)[:k]\n",
    "    if relevances.size == 0:\n",
    "        return 0.0\n",
    "    dcg = np.sum((2 ** relevances - 1) / np.log2(np.arange(2, relevances.size + 2)))\n",
    "    ideal_relevances = np.sort(relevances)[::-1]\n",
    "    idcg = np.sum((2 ** ideal_relevances - 1) / np.log2(np.arange(2, ideal_relevances.size + 2)))\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "ndcgs = []\n",
    "for u in user_preds:\n",
    "    preds_u = np.array(user_preds[u])\n",
    "    truths_u = np.array(user_truth[u])\n",
    "    relevances = (truths_u >= 4.0).astype(int)\n",
    "    top_k_indices = np.argsort(-preds_u)[:k]\n",
    "    ndcg_u = ndcg_at_k(relevances[top_k_indices], k)\n",
    "    ndcgs.append(ndcg_u)\n",
    "\n",
    "ndcg_at_k_value = np.mean(ndcgs)\n",
    "\n",
    "print(f\"🎯 Precision@{k}: {precision_at_k:.4f}\")\n",
    "print(f\"📈 NDCG@{k}: {ndcg_at_k_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@10: 0.6998\n",
      "F1@10: 0.6612\n",
      "Métricas añadidas a 'model_metrics.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Calcular Recall@10 y F1@10\n",
    "recalls = []\n",
    "for u in user_preds:\n",
    "    preds_u = np.array(user_preds[u])\n",
    "    truths_u = np.array(user_truth[u])\n",
    "    total_relevant = np.sum(truths_u >= 4.0)\n",
    "    if total_relevant == 0:\n",
    "        continue\n",
    "    top_k_indices = np.argsort(-preds_u)[:10]\n",
    "    relevant_top_k = np.sum(truths_u[top_k_indices] >= 4.0)\n",
    "    recalls.append(relevant_top_k / total_relevant)\n",
    "\n",
    "recall_at_k = np.mean(recalls)\n",
    "f1_at_k = (\n",
    "    2 * (precision_at_k * recall_at_k) / (precision_at_k + recall_at_k)\n",
    "    if (precision_at_k + recall_at_k) > 0 else 0.0\n",
    ")\n",
    "\n",
    "print(f\"Recall@10: {recall_at_k:.4f}\")\n",
    "print(f\"F1@10: {f1_at_k:.4f}\")\n",
    "\n",
    "# Preparar entrada para el CSV final\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "metrics = {\n",
    "    \"Modelo\": \"NeuMF (1M)\",\n",
    "    \"Test RMSE\": float(np.round(rmse, 4)),\n",
    "    \"Test MAE\": float(np.round(mae, 4)),\n",
    "    \"Test R2\": float(np.round(r2, 4)),\n",
    "    \"Precision@10\": float(np.round(precision_at_k, 4)),\n",
    "    \"Recall@10\": float(np.round(recall_at_k, 4)),\n",
    "    \"F1@10\": float(np.round(f1_at_k, 4)),\n",
    "    \"NDCG@10\": float(np.round(ndcg_at_k_value, 4)),\n",
    "    }\n",
    "\n",
    "# Guardar en 'model_metrics.csv'\n",
    "metrics_df = pd.DataFrame([metrics])\n",
    "csv_path = \"../../performance/model_evaluations.csv\"\n",
    "\n",
    "try:\n",
    "    all_metrics_df = pd.read_csv(csv_path)\n",
    "    if all_metrics_df.empty:\n",
    "        all_metrics_df = pd.DataFrame(columns=metrics_df.columns)\n",
    "except (FileNotFoundError, pd.errors.EmptyDataError):\n",
    "    all_metrics_df = pd.DataFrame(columns=metrics_df.columns)\n",
    "\n",
    "all_metrics_df = pd.concat([all_metrics_df, metrics_df], ignore_index=True)\n",
    "all_metrics_df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(\"Métricas añadidas a 'model_metrics.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
