{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Assignment 2***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Import neccessary libraries and load the ratings***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   UserID  MovieID  Rating  Timestamp\n",
      "0       1     1193       5  978300760\n",
      "1       1      661       3  978302109\n",
      "2       1      914       3  978301968\n",
      "3       1     3408       4  978300275\n",
      "4       1     2355       5  978824291\n",
      "(1000209, 4)\n",
      "Index(['UserID', 'MovieID', 'Rating', 'Timestamp'], dtype='object')\n",
      "Usuarios después de filtrar: 6040\n",
      "Películas después de filtrar: 3416\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el archivo ratings.dat especificando el separador '::'\n",
    "ratings = pd.read_csv(\"../../data/ml-1m/ratings.dat\", sep=\"::\", engine=\"python\", header=None,\n",
    "                      names=[\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"])\n",
    "\n",
    "# Mostrar las primeras filas para verificar la carga\n",
    "print(ratings.head())\n",
    "print(ratings.shape)\n",
    "print(ratings.columns)\n",
    "\n",
    "# Filtro: usuarios con al menos 5 ratings\n",
    "user_counts = ratings['UserID'].value_counts()\n",
    "ratings = ratings[ratings['UserID'].isin(user_counts[user_counts >= 5].index)]\n",
    "\n",
    "# Filtro: películas con al menos 5 ratings\n",
    "movie_counts = ratings['MovieID'].value_counts()\n",
    "ratings = ratings[ratings['MovieID'].isin(movie_counts[movie_counts >= 5].index)]\n",
    "\n",
    "print(f\"Usuarios después de filtrar: {ratings['UserID'].nunique()}\")\n",
    "print(f\"Películas después de filtrar: {ratings['MovieID'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Rating  rating_norm  rating_std\n",
      "0       5          1.0    1.269615\n",
      "1       3          0.6   -0.521065\n",
      "2       3          0.6   -0.521065\n",
      "3       4          0.8    0.374275\n",
      "4       5          1.0    1.269615\n",
      "Media original: 3.5820 | Desviación: 1.1169\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Mapeo de IDs con LabelEncoder (más ordenado y reutilizable)\n",
    "user_encoder = LabelEncoder()\n",
    "movie_encoder = LabelEncoder()\n",
    "\n",
    "ratings['userIndex'] = user_encoder.fit_transform(ratings['UserID'])\n",
    "ratings['movieIndex'] = movie_encoder.fit_transform(ratings['MovieID'])\n",
    "\n",
    "# Normalización a [0, 1]\n",
    "ratings['rating_norm'] = ratings['Rating'] / 5.0\n",
    "\n",
    "# Estandarización (media 0, desviación 1)\n",
    "mean_rating = ratings['Rating'].mean()\n",
    "std_rating = ratings['Rating'].std()\n",
    "ratings['rating_std'] = (ratings['Rating'] - mean_rating) / std_rating\n",
    "\n",
    "# Mostrar resumen\n",
    "print(ratings[['Rating', 'rating_norm', 'rating_std']].head())\n",
    "print(f\"Media original: {mean_rating:.4f} | Desviación: {std_rating:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Timestamp            datetime  year  month  dayofweek\n",
      "0  978300760 2000-12-31 22:12:40  2000     12          6\n",
      "1  978302109 2000-12-31 22:35:09  2000     12          6\n",
      "2  978301968 2000-12-31 22:32:48  2000     12          6\n",
      "3  978300275 2000-12-31 22:04:35  2000     12          6\n",
      "4  978824291 2001-01-06 23:38:11  2001      1          5\n"
     ]
    }
   ],
   "source": [
    "# Convertir timestamp a datetime\n",
    "ratings['datetime'] = pd.to_datetime(ratings['Timestamp'], unit='s')\n",
    "\n",
    "# Extraer año, mes y día de la semana\n",
    "ratings['year'] = ratings['datetime'].dt.year\n",
    "ratings['month'] = ratings['datetime'].dt.month\n",
    "ratings['dayofweek'] = ratings['datetime'].dt.dayofweek  # 0=Lunes, 6=Domingo\n",
    "\n",
    "# Mostrar resumen\n",
    "print(ratings[['Timestamp', 'datetime', 'year', 'month', 'dayofweek']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Import neccessary libraries and load the movies***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MovieID                               Title                        Genres\n",
      "0        1                    Toy Story (1995)   Animation|Children's|Comedy\n",
      "1        2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
      "2        3             Grumpier Old Men (1995)                Comedy|Romance\n",
      "3        4            Waiting to Exhale (1995)                  Comedy|Drama\n",
      "4        5  Father of the Bride Part II (1995)                        Comedy\n",
      "(3883, 3)\n",
      "Index(['MovieID', 'Title', 'Genres'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el archivo movies.dat con codificación ISO-8859-1 y separador '::'\n",
    "movies = pd.read_csv(\"../../data/ml-1m/movies.dat\", sep=\"::\", engine=\"python\", header=None,\n",
    "                     names=[\"MovieID\", \"Title\", \"Genres\"], encoding=\"latin1\")\n",
    "\n",
    "# Mostrar las primeras filas para verificar la carga\n",
    "print(movies.head())\n",
    "print(movies.shape)\n",
    "print(movies.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Unir ratings y Movies***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MovieID                                   Title  \\\n",
      "0     1193  One Flew Over the Cuckoo's Nest (1975)   \n",
      "1      661        James and the Giant Peach (1996)   \n",
      "2      914                     My Fair Lady (1964)   \n",
      "3     3408                  Erin Brockovich (2000)   \n",
      "4     2355                    Bug's Life, A (1998)   \n",
      "\n",
      "                         Genres  \n",
      "0                         Drama  \n",
      "1  Animation|Children's|Musical  \n",
      "2               Musical|Romance  \n",
      "3                         Drama  \n",
      "4   Animation|Children's|Comedy  \n"
     ]
    }
   ],
   "source": [
    "# Unir ratings con movies usando MovieID\n",
    "ratings = ratings.merge(movies, on=\"MovieID\", how=\"inner\")\n",
    "\n",
    "# Mostrar algunas columnas nuevas para comprobar\n",
    "print(ratings[['MovieID', 'Title', 'Genres']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***one hot encoding para los generos***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
      "                                    Title  Action  Adventure  Animation  \\\n",
      "0  One Flew Over the Cuckoo's Nest (1975)       0          0          0   \n",
      "1        James and the Giant Peach (1996)       0          0          1   \n",
      "2                     My Fair Lady (1964)       0          0          0   \n",
      "\n",
      "   Children's  Comedy  Crime  Documentary  Drama  Fantasy  Film-Noir  Horror  \\\n",
      "0           0       0      0            0      1        0          0       0   \n",
      "1           1       0      0            0      0        0          0       0   \n",
      "2           0       0      0            0      0        0          0       0   \n",
      "\n",
      "   Musical  Mystery  Romance  Sci-Fi  Thriller  War  Western  \n",
      "0        0        0        0       0         0    0        0  \n",
      "1        1        0        0       0         0    0        0  \n",
      "2        1        0        1       0         0    0        0  \n"
     ]
    }
   ],
   "source": [
    "# Separar los géneros por '|' y obtener one-hot encoding\n",
    "genres_onehot = ratings['Genres'].str.get_dummies(sep='|')\n",
    "\n",
    "# Añadir los géneros al dataframe de ratings\n",
    "ratings = pd.concat([ratings, genres_onehot], axis=1)\n",
    "\n",
    "# Mostrar las columnas de género\n",
    "print(genres_onehot.columns.tolist())\n",
    "print(ratings[['Title'] + genres_onehot.columns.tolist()].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Extraer los generos***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([999611, 18])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "genre_columns = genres_onehot.columns.tolist()\n",
    "\n",
    "# Extraer vectores de género para cada entrada\n",
    "genre_vectors = torch.tensor(ratings[genre_columns].values, dtype=torch.float32)\n",
    "\n",
    "# Mostrar ejemplo\n",
    "print(genre_vectors.shape)\n",
    "print(genre_vectors[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***dataset personalizado***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self, users, movies, genres, ratings):\n",
    "        self.users = users\n",
    "        self.movies = movies\n",
    "        self.genres = genres\n",
    "        self.ratings = ratings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'user': self.users[idx],\n",
    "            'movie': self.movies[idx],\n",
    "            'genre': self.genres[idx],\n",
    "            'rating': self.ratings[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Elegimos columna de rating\n",
    "rating_col = 'rating_norm'  # o 'rating_std'\n",
    "\n",
    "# Filtrar usuarios con al menos 3 ratings\n",
    "user_counts = ratings['UserID'].value_counts()\n",
    "ratings_filtered = ratings[ratings['UserID'].isin(user_counts[user_counts >= 3].index)]\n",
    "\n",
    "# Nuevo split\n",
    "train_list, val_list, test_list = [], [], []\n",
    "\n",
    "for user_id, group in ratings_filtered.groupby('UserID'):\n",
    "    user_train, user_temp = train_test_split(group, test_size=0.30, random_state=42)\n",
    "    user_val, user_test = train_test_split(user_temp, test_size=0.50, random_state=42)\n",
    "    train_list.append(user_train)\n",
    "    val_list.append(user_val)\n",
    "    test_list.append(user_test)\n",
    "\n",
    "train_data = pd.concat(train_list).reset_index(drop=True)\n",
    "val_data = pd.concat(val_list).reset_index(drop=True)\n",
    "test_data = pd.concat(test_list).reset_index(drop=True)\n",
    "\n",
    "# Extraer tensores (incluyendo géneros ahora sí)\n",
    "train_user = torch.tensor(train_data['userIndex'].values, dtype=torch.long)\n",
    "train_movie = torch.tensor(train_data['movieIndex'].values, dtype=torch.long)\n",
    "train_rating = torch.tensor(train_data[rating_col].values, dtype=torch.float32)\n",
    "train_genres = torch.tensor(train_data[genre_columns].values, dtype=torch.float32)\n",
    "\n",
    "val_user = torch.tensor(val_data['userIndex'].values, dtype=torch.long)\n",
    "val_movie = torch.tensor(val_data['movieIndex'].values, dtype=torch.long)\n",
    "val_rating = torch.tensor(val_data[rating_col].values, dtype=torch.float32)\n",
    "val_genres = torch.tensor(val_data[genre_columns].values, dtype=torch.float32)\n",
    "\n",
    "test_user = torch.tensor(test_data['userIndex'].values, dtype=torch.long)\n",
    "test_movie = torch.tensor(test_data['movieIndex'].values, dtype=torch.long)\n",
    "test_rating = torch.tensor(test_data[rating_col].values, dtype=torch.float32)\n",
    "test_genres = torch.tensor(test_data[genre_columns].values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset personalizado ya lo definiste antes (con genre incluido)\n",
    "\n",
    "train_dataset = MovieLensDataset(train_user, train_movie, train_genres, train_rating)\n",
    "val_dataset = MovieLensDataset(val_user, val_movie, val_genres, val_rating)\n",
    "test_dataset = MovieLensDataset(test_user, test_movie, test_genres, test_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 512  # Puedes ajustarlo según tu GPU\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuMF(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, num_genres, embedding_dim_gmf=32, embedding_dim_mlp=32, dropout=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embeddings GMF\n",
    "        self.user_embedding_gmf = nn.Embedding(num_users, embedding_dim_gmf)\n",
    "        self.movie_embedding_gmf = nn.Embedding(num_movies, embedding_dim_gmf)\n",
    "\n",
    "        # Embeddings MLP\n",
    "        self.user_embedding_mlp = nn.Embedding(num_users, embedding_dim_mlp)\n",
    "        self.movie_embedding_mlp = nn.Embedding(num_movies, embedding_dim_mlp)\n",
    "\n",
    "        # Mini-MLP para géneros\n",
    "        self.genre_layer = nn.Sequential(\n",
    "            nn.Linear(num_genres, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # MLP principal\n",
    "        self.fc1 = nn.Linear(embedding_dim_mlp * 2 + 32, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Capa final (concatena GMF + MLP)\n",
    "        self.output = nn.Linear(embedding_dim_gmf + 64, 1)\n",
    "\n",
    "    def forward(self, user, movie, genre):\n",
    "        # GMF\n",
    "        user_gmf = self.user_embedding_gmf(user)\n",
    "        movie_gmf = self.movie_embedding_gmf(movie)\n",
    "        gmf_output = user_gmf * movie_gmf\n",
    "\n",
    "        # MLP\n",
    "        user_mlp = self.user_embedding_mlp(user)\n",
    "        movie_mlp = self.movie_embedding_mlp(movie)\n",
    "        genre_repr = self.genre_layer(genre)\n",
    "\n",
    "        mlp_input = torch.cat([user_mlp, movie_mlp, genre_repr], dim=1)\n",
    "        x = F.relu(self.bn1(self.fc1(mlp_input)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Final\n",
    "        final_input = torch.cat([gmf_output, x], dim=1)\n",
    "        out = self.output(final_input)\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "model = NeuMF(\n",
    "    num_users=len(user_encoder.classes_),\n",
    "    num_movies=len(movie_encoder.classes_),\n",
    "    num_genres=len(genre_columns),\n",
    "    embedding_dim_gmf=32,\n",
    "    embedding_dim_mlp=32,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "# Pérdida\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizador\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Scheduler: reduce el LR si no mejora la validación\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Train Loss: 0.0618 | Val Loss: 0.0438\n",
      "Epoch 2/30 | Train Loss: 0.0408 | Val Loss: 0.0358\n",
      "Epoch 3/30 | Train Loss: 0.0360 | Val Loss: 0.0346\n",
      "Epoch 4/30 | Train Loss: 0.0349 | Val Loss: 0.0343\n",
      "Epoch 5/30 | Train Loss: 0.0340 | Val Loss: 0.0331\n",
      "Epoch 6/30 | Train Loss: 0.0332 | Val Loss: 0.0324\n",
      "Epoch 7/30 | Train Loss: 0.0325 | Val Loss: 0.0320\n",
      "Epoch 8/30 | Train Loss: 0.0320 | Val Loss: 0.0315\n",
      "Epoch 9/30 | Train Loss: 0.0316 | Val Loss: 0.0314\n",
      "Epoch 10/30 | Train Loss: 0.0312 | Val Loss: 0.0310\n",
      "Epoch 11/30 | Train Loss: 0.0309 | Val Loss: 0.0312\n",
      "Epoch 12/30 | Train Loss: 0.0306 | Val Loss: 0.0308\n",
      "Epoch 13/30 | Train Loss: 0.0302 | Val Loss: 0.0307\n",
      "Epoch 14/30 | Train Loss: 0.0300 | Val Loss: 0.0305\n",
      "Epoch 15/30 | Train Loss: 0.0297 | Val Loss: 0.0304\n",
      "Epoch 16/30 | Train Loss: 0.0296 | Val Loss: 0.0304\n",
      "Epoch 17/30 | Train Loss: 0.0294 | Val Loss: 0.0303\n",
      "Epoch 18/30 | Train Loss: 0.0292 | Val Loss: 0.0302\n",
      "Epoch 19/30 | Train Loss: 0.0290 | Val Loss: 0.0302\n",
      "Epoch 20/30 | Train Loss: 0.0289 | Val Loss: 0.0301\n",
      "Epoch 21/30 | Train Loss: 0.0288 | Val Loss: 0.0301\n",
      "Epoch 22/30 | Train Loss: 0.0287 | Val Loss: 0.0301\n",
      "Epoch 23/30 | Train Loss: 0.0286 | Val Loss: 0.0300\n",
      "Epoch 24/30 | Train Loss: 0.0285 | Val Loss: 0.0301\n",
      "Epoch 25/30 | Train Loss: 0.0284 | Val Loss: 0.0301\n",
      "Epoch 26/30 | Train Loss: 0.0283 | Val Loss: 0.0302\n",
      "Epoch 27/30 | Train Loss: 0.0269 | Val Loss: 0.0300\n",
      "Epoch 28/30 | Train Loss: 0.0265 | Val Loss: 0.0300\n",
      "Epoch 29/30 | Train Loss: 0.0262 | Val Loss: 0.0300\n",
      "Epoch 30/30 | Train Loss: 0.0251 | Val Loss: 0.0302\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "early_stopping_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        users = batch['user'].to(device)\n",
    "        movies = batch['movie'].to(device)\n",
    "        genres = batch['genre'].to(device)\n",
    "        ratings = batch['rating'].to(device)\n",
    "\n",
    "        preds = model(users, movies, genres)\n",
    "        loss = criterion(preds, ratings)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * len(ratings)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader.dataset)\n",
    "\n",
    "    # VALIDACIÓN\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            users = batch['user'].to(device)\n",
    "            movies = batch['movie'].to(device)\n",
    "            genres = batch['genre'].to(device)\n",
    "            ratings = batch['rating'].to(device)\n",
    "\n",
    "            preds = model(users, movies, genres)\n",
    "            loss = criterion(preds, ratings)\n",
    "            val_loss += loss.item() * len(ratings)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # EARLY STOPPING\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        early_stopping_counter = 0\n",
    "        #torch.save(model.state_dict(), \"best_model_with_genres.pth\")\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(\"Early stopping activado.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from collections import defaultdict\n",
    "\n",
    "# Medir tiempo\n",
    "start_time = time.time()\n",
    "\n",
    "# Cargar mejor modelo\n",
    "model.eval()\n",
    "\n",
    "# Evaluación sobre el set de test\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        users = batch['user'].to(device)\n",
    "        movies = batch['movie'].to(device)\n",
    "        genres = batch['genre'].to(device)\n",
    "        ratings = batch['rating'].to(device)\n",
    "\n",
    "        preds = model(users, movies, genres)\n",
    "\n",
    "        # Desnormalizar si es necesario\n",
    "        preds = preds * 5\n",
    "        ratings = ratings * 5\n",
    "\n",
    "        y_true.extend(ratings.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "# Precision@10 y NDCG@10\n",
    "k = 10\n",
    "user_preds = defaultdict(list)\n",
    "user_truth = defaultdict(list)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        users = batch['user'].to(device)\n",
    "        movies = batch['movie'].to(device)\n",
    "        genres = batch['genre'].to(device)\n",
    "        ratings = batch['rating'].to(device)\n",
    "        preds = model(users, movies, genres)\n",
    "\n",
    "        preds = preds * 5\n",
    "        ratings = ratings * 5\n",
    "\n",
    "        for u, pred, true in zip(users.cpu().numpy(), preds.cpu().numpy(), ratings.cpu().numpy()):\n",
    "            user_preds[u].append(pred)\n",
    "            user_truth[u].append(true)\n",
    "\n",
    "precisions = []\n",
    "for u in user_preds:\n",
    "    preds_u = np.array(user_preds[u])\n",
    "    truths_u = np.array(user_truth[u])\n",
    "    top_k_indices = np.argsort(-preds_u)[:k]\n",
    "    relevant = (truths_u >= 4.0)\n",
    "    precisions.append(np.sum(relevant[top_k_indices]) / k)\n",
    "precision_at_k = np.mean(precisions)\n",
    "\n",
    "def ndcg_at_k(relevances, k):\n",
    "    relevances = np.asarray(relevances)[:k]\n",
    "    if relevances.size == 0:\n",
    "        return 0.0\n",
    "    dcg = np.sum((2 ** relevances - 1) / np.log2(np.arange(2, relevances.size + 2)))\n",
    "    idcg = np.sum((2 ** np.sort(relevances)[::-1] - 1) / np.log2(np.arange(2, relevances.size + 2)))\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "ndcgs = []\n",
    "for u in user_preds:\n",
    "    preds_u = np.array(user_preds[u])\n",
    "    truths_u = np.array(user_truth[u])\n",
    "    relevances = (truths_u >= 4.0).astype(int)\n",
    "    top_k_indices = np.argsort(-preds_u)[:k]\n",
    "    ndcgs.append(ndcg_at_k(relevances[top_k_indices], k))\n",
    "ndcg_at_k_value = np.mean(ndcgs)\n",
    "\n",
    "# Tiempo total\n",
    "total_training_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@10: 0.7129\n",
      "F1@10: 0.6788\n",
      "Métricas añadidas correctamente a 'model_metrics.csv'\n"
     ]
    }
   ],
   "source": [
    "# Calcular Recall@10 y F1@10\n",
    "recalls = []\n",
    "for u in user_preds:\n",
    "    preds_u = np.array(user_preds[u])\n",
    "    truths_u = np.array(user_truth[u])\n",
    "    total_relevant = np.sum(truths_u >= 4.0)\n",
    "    if total_relevant == 0:\n",
    "        continue\n",
    "    top_k_indices = np.argsort(-preds_u)[:10]\n",
    "    relevant_top_k = np.sum(truths_u[top_k_indices] >= 4.0)\n",
    "    recall_u = relevant_top_k / total_relevant\n",
    "    recalls.append(recall_u)\n",
    "\n",
    "recall_at_k = np.mean(recalls)\n",
    "f1_at_k = (\n",
    "    2 * (precision_at_k * recall_at_k) / (precision_at_k + recall_at_k)\n",
    "    if (precision_at_k + recall_at_k) > 0 else 0.0\n",
    ")\n",
    "\n",
    "print(f\"Recall@10: {recall_at_k:.4f}\")\n",
    "print(f\"F1@10: {f1_at_k:.4f}\")\n",
    "\n",
    "# Guardar todo en el CSV conjunto\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "metrics = {\n",
    "    \"Modelo\": \"NeuMF + Genres (1M)\",\n",
    "    \"Test RMSE\": float(np.round(rmse, 4)),\n",
    "    \"Test MAE\": float(np.round(mae, 4)),\n",
    "    \"Test R2\": float(np.round(r2, 4)),\n",
    "    \"Precision@10\": float(np.round(precision_at_k, 4)),\n",
    "    \"Recall@10\": float(np.round(recall_at_k, 4)),\n",
    "    \"F1@10\": float(np.round(f1_at_k, 4)),\n",
    "    \"NDCG@10\": float(np.round(ndcg_at_k_value, 4)),\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame([metrics])\n",
    "csv_path = \"../../performance/model_evaluations.csv\"\n",
    "\n",
    "try:\n",
    "    all_metrics_df = pd.read_csv(csv_path)\n",
    "    if all_metrics_df.empty:\n",
    "        all_metrics_df = pd.DataFrame(columns=metrics_df.columns)\n",
    "except (FileNotFoundError, pd.errors.EmptyDataError):\n",
    "    all_metrics_df = pd.DataFrame(columns=metrics_df.columns)\n",
    "\n",
    "all_metrics_df = pd.concat([all_metrics_df, metrics_df], ignore_index=True)\n",
    "all_metrics_df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(\"Métricas añadidas correctamente a 'model_metrics.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
